{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO+0rQsNyVXkkIEh1rEiucE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["-> Unsupervised meaning we have input features but we don't have the labels\n","\n","eg: Suppose we want to create a system that will take a few pictures of each item on a manufacturing production line and detect which items are defective.\n","\n","-> Unsupervised Learning tasks\n","\n","1. Dimensionality reduction\n","\n","Clustering for dimesnionality reduction - Once a dataset has been clustered, its usually possible to measure each instance's affinity with each cluster. affinity is any measure of how well an instance fits into a cluster. Each instance's feature vector x can then be replaced with the vector of its cluster affinities. if there are k clusters then this vector is k-dimensional. New vector is typically much lower dimensional than the original feature vector, but it can preserve enough information for further processing.\n","\n","2. Clustering\n","\n","Goal is to group similar instances together into clusters. Clustering is a great tool for data analysis, customer segmentation, recommender systems, search engines (To search for images similar to a reference image. First apply a clustering algorithm to all the images in the database. SImilar images will form a cluster), image segmentation, semi-supervised learning (If we have only few labels, do clustering and propagate the labels to all the instances in the same cluster), dimensionality reduction etc.\n","\n","eg: k-means,DBSCAN\n","\n","3. Anomaly (or Outlier) detection\n","\n","Objective is to learn what normal data looks like and then use that to detect abnormal instances. These instances are called anomalies or outliers while normal instances are called inliers. Useful in fraud detection, detecting defective products in manufacturing, identifying new trends in time series, removing outliers from dataset before training another model which can significantly improve the performance of the resulting model. Using clustering, any instance with low affinity to all the clusters is likely to be an anomaly.\n","\n","4. Density estimation\n","\n","Task of estimating probability density function (PDF) of the random process that generated the dataset. Commonly used for anomaly detection. Instances located in very low density regions are likely to be anomalies. Useful for data analysis and visualization."],"metadata":{"id":"XTGAqp2YBs-F"}},{"cell_type":"markdown","source":["# k-means (Lloyd-Forgy algorithm)\n","\n","-> One of the fastest clustering algorithms\n","\n","-> Start by placing the centroids randomly eg: by picking k instances at random from the dataset and using their locations as centroids. Then label the instances - assigning to the cluster whose centroid is the closest , update the centroids - by computing the mean of the instances in that cluster, and so on until the centroid stops moving. Algorithm is guaranteed to converge because the mean squared distance between the instances and their closest centroids can only go down at each step and since it cannot be negative, its guaranteed to converge.\n","\n","-> Important to scale input featuresbefore running k-means otherwise clusters may be very stretched and kmeans will perform poorly.\n","\n","-> Hard clustering - Each instance assigned to single cluster\n","\n","-> Soft clustering - Give each instance a score per cluster. Score - distance from centroid or similarity score like Gaussian radial basis function\n","\n","-> Centrid initialization methods\n","\n","1. If we know approx where the centroids should be\n","\n","good_init=np.array([[-3,3],[-3,2],[-3,1],[-1,2],[0,2]])\n","\n","kmeans=KMeans(n_clusters=5,init=good_init,n_init=1,random_state=42)\n","\n","kmeans.fit(X)\n","\n","2. Run algorithm multiple times with different random initializations and keep the best solution. Number of random initializations is controlled by n_init hyperparameter. By default its 10. To determine the best solution, model's inertia which is the sum of the squared distances between the instances and their closest centroids is used as performance metric (WCSS - Within cluster sum of squares. Its a measure of variability of data points within each cluster).\n","\n","WCSS = sigma i=1 to k sigma j=1 to mi (Xij-Ci)^2\n","\n","-> Finding optimal number of clusters\n","\n","When k is too small => separate clusters get merged.\n","\n","When k is too large => Some clusters get chopped into multiple pieces.\n","\n","As we increase k inertia will be reduced. More clusters there are closest each instance will be to its closest centroid so less inertia. So its not a good performance metric in choosing number of clusters. Plot inertia as a function of k. Curve will have an elbow point.Inertia drops very quickly as we increase k till thatb point after that inertia decreases very slowly.\n","\n","Silhouette score - Mean Silhouette coefficient over all the instances.\n","\n","Silhouette coefficient = (b-a)/max(a,b) where a=mean intra cluster distance ie mean distances to other instances in the same cluster , b=mean nearest cluster distance ie mean distance to the instances of the nest closest cluster excluding instance's own cluster. It can vary from -1 to +1.\n","\n","+1 => Instance is well inside its own cluster and far from other clusters.\n","\n","0=> Its close to a cluster boundary\n","\n","-1 => It may have been assigned to wrong cluster.\n","\n","Silhouette diagram - Plot every instance's silhouette coefficient sorted by the clusters they are assigned to and by the value of the coefficient. Each diagram contains one knife shape per cluster. Shape's height indicates number of instances in that cluster. Width represents sorted silhouette coefficients of the instances in the cluster. Wider is better.\n","\n","-> Drawbacks of k-means\n","\n","1. May get bad results because of random initialization of the centroids\n","\n","2. May not give the best results for data where the clusters are of varying size or density.\n","\n","3. Number of clusters (k) is not defined\n","\n","-> Accelerated k-means\n","\n","On large datasets with many clusters, algorithm can be accelerated by avoiding many unnecessary distance calculations. Exploiting triangle inequality and by keeping track of lower and upper bounds for distances between instances and centroids. algorithm='elkan'\n","\n","-> Mini batch k-means\n","\n","Instead of using full datset, using mini batches moving the centroids slightly at each iteration. Speeds up the algorithm Makes it possible to cluster huge datasets that do not fit in memory. It will have higher inertia than k-means but faster.\n","\n","from sklearn.cluster import MiniBatchKMeans\n","\n","minibatch_kmeans=MiniBatchKMeans(n_Clusters=5,random_state=42)\n","\n","minibatch_kmeans.fit(X)\n","\n","# K-means++\n","\n","Sklearn by default uses k-means++.\n","Initialization step tends to select centroids that are distant from one another. This makes k=means algo much less likely to converge to suboptimal solution.\n","\n","1. Take one centroid randomly from dataset.\n","\n","2. Take new centroid choosing an instance xi with probability D(xi)^2/sigma j=1 to m D(xj)^2 . Instances far away from already chosen centroids are much more likely to be selected.\n","\n","3. Repeat 2 until k centroids have been chosen.\n","\n","-> Limitations\n","\n","Its necessary to run algorithm multiple times to avaoid suboptimal solutions plus we need to specify number of clusters. When clusters have varying sizes different densities nonspherical shapes, kmeans doesnt behave very well.\n","\n","# Hierarchical clustering\n","\n","Hierarchy of clusters is developed in the form of a tree and this tree shaped structure is known as the dendogram.\n","\n","1. Compute the distance (Euclidean, Manhattan, Hamming Distance) between all the data points.\n","\n","2. Merge 2 points having minimum distance between them.\n","\n","3. Treat the subclusters as points and repeat 2 util one cluster remains.\n","\n","4. Finally develop the dendogram to divide the clusters as per the problem.\n","\n","Common type of linkages between 2 clusters are\n","\n","-> Complete Linkage - Maximum distance b/w 2 farthest points\n","\n","-> Single Linkage - Minimum distance b/w 2 closest points\n","\n","-> Centroid Linkage - Distance b/w centroids\n","\n","-> Ward's distance/Linkage - b/w 2 clusters c1 and c2 is\n","\n","W(c1+c2) = WCSS(c1+c2)-WCSS(c1)-WCSS(C2)\n","\n","No need to pre-decide k value. Good for data visualization providing hierarchical relation b/w the clusters.\n","\n","This method is sensitive to the choice of linkage function. It can be used only for analysis not for making inferences. If we get a new data point we need to rerun the entire algorithm with all the existing points and reanalyze.\n"],"metadata":{"id":"L57ieiaYVyO1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"44B-2DDs_gaG"},"outputs":[],"source":["from sklearn.cluster import KMeans\n","from sklearn.datasets import make_blobs\n","import numpy as np\n","\n","blob_centers = np.array([[ 0.2,  2.3], [-1.5 ,  2.3], [-2.8,  1.8],\n","                         [-2.8,  2.8], [-2.8,  1.3]])\n","blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n","X, y = make_blobs(n_samples=2000, centers=blob_centers, cluster_std=blob_std,\n","                  random_state=7)"]},{"cell_type":"code","source":["k=5\n","kmeans=KMeans(n_clusters=k,random_state=42)\n","y_pred=kmeans.fit_predict(X)\n","y_pred"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fa9A-P9pGrzd","executionInfo":{"status":"ok","timestamp":1709198349252,"user_tz":-330,"elapsed":31,"user":{"displayName":"Saipriya M Ravi","userId":"01294885343977700205"}},"outputId":"036a3f7e-807b-4244-8448-cd75fde7c3e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["array([4, 0, 1, ..., 2, 1, 0], dtype=int32)"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["kmeans.cluster_centers_"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n8ckHP26G4LJ","executionInfo":{"status":"ok","timestamp":1709198349252,"user_tz":-330,"elapsed":30,"user":{"displayName":"Saipriya M Ravi","userId":"01294885343977700205"}},"outputId":"103edeb2-cd13-4e62-e717-80fc1f23e81d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-2.80389616,  1.80117999],\n","       [ 0.20876306,  2.25551336],\n","       [-2.79290307,  2.79641063],\n","       [-1.46679593,  2.28585348],\n","       [-2.80037642,  1.30082566]])"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["X_new=np.array([[0,2],[3,2],[-3,3],[-3,2.5]])\n","kmeans.predict(X_new)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JQDadjSeHNJq","executionInfo":{"status":"ok","timestamp":1709198349253,"user_tz":-330,"elapsed":28,"user":{"displayName":"Saipriya M Ravi","userId":"01294885343977700205"}},"outputId":"f0fcd861-8f6b-4f74-9ac2-d904aa0aca38"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 1, 2, 2], dtype=int32)"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["from sklearn.metrics import silhouette_score\n","silhouette_score(X,kmeans.labels_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eHkfY3YtH81q","executionInfo":{"status":"ok","timestamp":1709198349253,"user_tz":-330,"elapsed":25,"user":{"displayName":"Saipriya M Ravi","userId":"01294885343977700205"}},"outputId":"e92f143e-58b6-49a2-c9ec-e584ab15ed69"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.655517642572828"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["# DBSCAN (Density based spatial clustering of applications with noise)\n","\n","1. For each instance, the algorithm counts how many instances are located within a small distance epsilon from it. This region is called the instance's epsilon-neighbourhood.\n","\n","2. If an instance has at least min_samples instances in its epsilon-neighbourhood then its considered a core instance. Core instances are those that are located in dense regions.\n","\n","3. Long sequence of core instances forms a single cluster. All instances in the neighborhood of a core instance belong to the same cluster.\n","\n","4. Any instance that is not a core instance and doesnt have one in its neighborhood is considered anomaly.\n","\n","Core point - Point that has atleast minPts points within distance eps from itself.\n","\n","Border point - Point that has atleast one Core point at a distance eps\n","\n","Noise point - Neither a core nor a border. And it has < minPts points within distance eps from itself.\n","\n","-> DBSCAN doesnt have predict(). It cannot predict which cluster a new instance belongs to. We can use different classification algos for that.\n","\n","-> Powerful algo capable of identifying any no of clusters of any shape.Robust to outliers. Doesn't work well with high dimensional data. DBSCAN struggles with clusters of similar density.\n","\n","-> 2 hyperparameters - epsilon,min_samples. One way to decide initial value of eps is by plotting histogram of distance between each point. We may get 2 peaks. Initial value of eps may be one of the distance values between these 2 peaks.\n","\n","1. Pick a random point from the dataset and if its a core point, assign it to cluster 1 and mark it as visited.\n","\n","2. If not, point is marked as noise. Assign all the points within the neighborhood of the initial point to cluster 1. If these new points are core points assign its neighborhood points also to cluster1.\n","\n","3. Next randomly choose another unvisited point and repeat step 1 with it.\n","\n","4. Stop when all points are visited."],"metadata":{"id":"hjCDKjZbA_8Z"}},{"cell_type":"code","source":["from sklearn.cluster import DBSCAN\n","from sklearn.datasets import make_moons\n","\n","X,y=make_moons(n_samples=1000,noise=0.05)\n","dbscan=DBSCAN(eps=0.05,min_samples=5)\n","dbscan.fit(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":75},"id":"XVywR7rWCu7H","executionInfo":{"status":"ok","timestamp":1709198349253,"user_tz":-330,"elapsed":24,"user":{"displayName":"Saipriya M Ravi","userId":"01294885343977700205"}},"outputId":"01021c89-1e8d-4ec2-db7a-1cafb22b1123"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DBSCAN(eps=0.05)"],"text/html":["<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DBSCAN(eps=0.05)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DBSCAN</label><div class=\"sk-toggleable__content\"><pre>DBSCAN(eps=0.05)</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["dbscan.labels_     ## labels of all instances\n","dbscan.core_sample_indices_  ## indices of core instances\n","dbscan.components_     ## core instances"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YwzuPzhVImJp","executionInfo":{"status":"ok","timestamp":1709198349254,"user_tz":-330,"elapsed":23,"user":{"displayName":"Saipriya M Ravi","userId":"01294885343977700205"}},"outputId":"a34c5462-d42c-4579-cb73-f2345d93fb2e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-0.67588346,  0.79528251],\n","       [ 1.95994091,  0.33687059],\n","       [ 1.99458094,  0.17977172],\n","       ...,\n","       [-1.0212242 ,  0.1376901 ],\n","       [ 1.94821479,  0.10318642],\n","       [-0.70707713,  0.65859321]])"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["from sklearn.neighbors import KNeighborsClassifier\n","knn=KNeighborsClassifier(n_neighbors=50)\n","knn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])\n","\n","## trained only on core instances. We could also have chosen to train it on all the instances or all but the anomalies. Choice depends on final task."],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":75},"id":"dfxSG4_aJJd3","executionInfo":{"status":"ok","timestamp":1709198349641,"user_tz":-330,"elapsed":409,"user":{"displayName":"Saipriya M Ravi","userId":"01294885343977700205"}},"outputId":"17d168e3-a4c6-41f3-f2c1-e8bdf319c1fd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["KNeighborsClassifier(n_neighbors=50)"],"text/html":["<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=50)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=50)</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["X_new=np.array([[-0.5,0],[0,0.5],[1,-0.1],[2,1]])\n","knn.predict(X_new)\n","knn.predict_proba(X_new)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pZkfTD0UJkgb","executionInfo":{"status":"ok","timestamp":1709198349641,"user_tz":-330,"elapsed":407,"user":{"displayName":"Saipriya M Ravi","userId":"01294885343977700205"}},"outputId":"354fb0f9-09c2-46c2-c5c5-59fbcf2c169f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.78, 0.  , 0.  , 0.22, 0.  , 0.  , 0.  , 0.  ],\n","       [0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  ],\n","       [0.72, 0.  , 0.14, 0.  , 0.14, 0.  , 0.  , 0.  ],\n","       [0.  , 0.76, 0.  , 0.  , 0.  , 0.16, 0.  , 0.08]])"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["# Gaussian Mixture Model (GMM)\n","\n","Its a probabilistic model that assumes that the instances were generated from a mixture of several Gaussian distributions whose paraemters are unknown.All the instances generated from a single Gaussian distribution form a cluster that typically looks like an ellipsoid. Each cluster can have a different ellipsoidal shape,size,density and orientation. It uses soft clustering approach for distributing the points in different clusters.\n","\n","-> **GMM Algorithm (1D)**\n","\n","1. Randomly initialize mu and sigma\n","\n","2. Update mu and sigma\n","\n","*   Calc prob of each pt belonging to each gaussian\n","*   Assign all the pts which have the highest prob to their resp gaussians\n","*   Compute new mu for each gaussian as the mean of the pts belonging to that gaussian\n","*   Similarly update the variance\n","*   After multiple updates, we will have tightly fitting Gaussian disbns representing different clusters.\n","\n","-> **2D/ND Gaussians**\n","\n","2 variables x and y. Multivariate disbn is defined by 5 parameters:\n","\n","mux-mean of x coordinate\n","\n","muy-mean of y coordinate\n","\n","sigmax^2-variance in x direction\n","\n","sigmay^2-variance in y direction\n","\n","Cov(x,y)-Covariance b/w x and y\n","\n","\n","\n","\n","\n"],"metadata":{"id":"mN1hzIZACvi3"}},{"cell_type":"markdown","source":["# Anomaly detection algorithms\n","\n","**1. RANSAC**\n","\n","A dataset with number of points having parameters mu and sigma^2\n","\n","-> Sample a subset of points (n). We consider this as inliers.\n","\n","-> Now compute a model that estimates parameters of the sampled points.\n","\n","-> Score the model which indicates how many points will support the model.\n","\n","Repeat above steps and select best model. That tells the points which are inliers and outliers.\n","\n","\n","**2. Elliptic Envelope**\n","\n","X follows normal distribution being unimodal. Elliptical envelope robustly estimate The parameters mu_vector,Sigma ()covariance matrix\n","\n","We remove the points that are outliers which are far away from the centroid.\n","\n","**3. Isolation Forests**\n","\n","Build many trees. For each tree,\n","\n","Randomly pick a feature, Randomly threshold that feature, Build each tree until the leaf consists of only one data point.\n","\n","On an average outliers have lower depth in random trees and inliers have more depth.\n","\n","**4. Local Outlier Factor**\n","\n","Based on 2 ideas - KNN, density. Compare the density of a point with its neighbor's density. If the density of a point is less than the density of its neighbors, we flag that point as an outlier. We compute density based on average distance. If average distance between  a point and its K nearest neighbors is large, its more likely that the point will be an outlier. Larger the value of K, more confident are the results.\n"],"metadata":{"id":"OBBiAVIA9qKa"}},{"cell_type":"code","source":[],"metadata":{"id":"H2D0nu7n-UfH"},"execution_count":null,"outputs":[]}]}