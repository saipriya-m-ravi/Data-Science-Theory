{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO64tvHUvHPjpnNtbCrY1XP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Curse of dimensionality\n","\n","Many ML problems involve thousands or even millions of features for each training instance. This makes training extremely slow and harder to find a good solution. THis is called curse of dimensionality.\n","\n","High dimensional datasets are at risk of being sparse. Most training instances are likely to be far way from each other. This also means that a new instance will likely be far away from any training instance making predictions much less reliable than in lower dimensions. More dimensions the training set has, the greater the risk of overfitting it. One solution could be to increase the training set size to reach a sufficient density of training instances. But the no of training instances required to reach a given density grows exponentially with the number of dimensions.\n","\n","## Why dimensionality reduction ?\n","\n","Reducing dimensionality cause information loss so training would be faster but it may degrade the performance.\n","\n","In some cases, reducing dimensionality of the training data may filter out some noise and unnecessary details and thus result in higher performance.\n","\n","Useful for data visualization.Reducing dimensions down to 2 or 3 makes it possible to plot a condensed view of a high dimensional training set on a graph and gain some important insights by visually detecting patterns such as clusters.\n","\n","2 main approaches for dimensionality reduction are projection and manifold learning.\n","\n","3 most popular dimensionality reduction techniques are PCA,random projection, locally linear embedding (LLE)\n","\n","## Projection\n","\n","In real world problems, training instances are not spread out uniformly across all dimensions. Many features are almost constant while others are highly correlated. As a result , all training instances lie within a much lower dimensional subspace of the high dimensional space.\n","\n","## Manifold Learning\n","\n","Cases where subspace may twist and turn eg: Swiss roll toy dataset - its an example of 2D manifold. 2D manifold is a 2D shape that can be bent and twisted in a higher dimensional space. Many dimensionality reduction algorithms work by modeling the manifold on which the training instances lie - manifold learning. It relies on the manifold assumption (or hypothesis) which holds that most real-world high dimensional datasets lie close to a much lower dimensional manifold. Task at hand (classification/regression) will be simpler if expressed in the lower dimensional space of the manifold.\n","\n","\n","Reducing dimensionality of training set before training a model will usually speed up training but it may not lead to a better or simpler solution."],"metadata":{"id":"t2IbIT6FCXMp"}},{"cell_type":"markdown","source":["# Principal Component Analysis (PCA)\n","\n","First it identifies the hyperplane that lies closest to the data and then it projects the data onto it. For example, to project a 2D dataset to a 1D hyperplane, select the axis that preserves the maximum amount of variance as it will most likely lose less information than the other projections. Or choose the axis that minimizes the mean squared distance between the original dataset and its projection onto that axis.\n","\n","Then it identifies a second axis orthogonal to the first one that accounts for the largest amount of remaining variance. In 2D example ther's no choice. There will be only one orthogonal axis. For higher dimensional dataset, there will be multiple orthogonal axes. Number of axes=Number of dimensions in the dataset. ith axis is called the ith principal component of the data.\n","\n","For each principal component PCA finds a zero-centered unit vector pointing in the direction of the PC. PCA assumes that the dataset is centered around the origin. Scikit-Learn's PCA classes take care of centering the data.\n","\n","To find the PC of training set, ther's a matrix factorization technique called singular value decomposition (SVD) that can decompose the training set matrix X into the matrix multiplication of 3 matrices U,epsilon,V_transpose where V contains the unit vectors that define all the PCs.\n","\n","The SVD factorization algorithm returns three matrices, U, Σ and V, such that X = UΣV⊺, where U is an m × m matrix, Σ is an m × n matrix, and V is an n × n matrix. But the svd() function returns U, s and V⊺ instead. s is the vector containing all the values on the main diagonal of the top n rows of Σ. Since Σ is full of zeros elsewhere, your can easily reconstruct it from s\n","\n","Once all PCs are identified, we can reduce the dimensionality of the dataset down to d dimensions by projecting it onto the hyperplane defined by first d PCs.To project the training set onto the hyperplane and obtain a reduced dataset Xd-proj of dimensionality d, compute the matrix multiplication of the training set matrix X by the matrix Wd, defined as the matrix containing the first d columns of V.\n","\n","Xd-proj = XWd\n","\n","-> explained_variance_ratio - proportion of dataset's variance that lies along each principal component.\n","\n","-> Right number of dimensions -\n","\n","1. Choose the no: of dimensions that add up to a significantly large portion of the variance say 95%\n","\n","2. If we are reducing dimensionality for data visualization, then we want t reduce it down to 2 or 3.\n","\n","3. Plot the explained variance as a function of the number of dimensions. There will usually be an elbow in the curve where explained variance stops growing fast.\n","\n","-> After dimensionality reduction, training set takes up much less space so faster training. Its also possible to decompress the reduced dataset back to original dimensions by applying the inverse transformation of the PCA projection. It wont give back the original dataset due to some information loss during projection but it will be close to original data. The mean squared distance between the original data and the reconstructed data is called reconstruction error.\n","\n","Xrecovered=Xd-proj Wd_transpose\n","\n","-> Complexity - O(m*n^2)+O(n^3)\n","\n","-> Randomized PCA - svd_solver=\"randomized\". Complexity - O(m*d^2)+O(d^3) Faster than full SVD when d is much smaller than n.\n","\n","-> Incremental PCA - It allows you to split the training set into mini batches and feed these in one mini batch at a time. This is useful for large training sets and for applying PCA online.\n","\n","-> If we are dealing with a dataset with tens of thousands of features or more , say images then training become much too slow. In that case consider random projection.\n","\n","## Maths behind it\n","\n","To find the best value of vector u, which is the direction of maximum variance or maximum information and along which we should rotate our existing coordinates, we follow the below-given steps\n","\n","1. Find the covariance matrix of feature matrix X\n","\n","2. Then calculate eigen vectors and eigen values of the covariance matrix. Eigen vector is the direction of best u and the eigen value is the importance of that vector.\n","\n","The eigen vector associated with the largest eigen value indicates the direction in which the data has the most variance. So we can select our pcs in the direction of the eigen vectors having large eigen values and drop the pcs having ralatively small eigen values.\n","\n","-> Eigen vector and eigen value\n","\n","For any matrix A, there exists x such that when this vector is multiplied with matrix A, we get a new vector in the same direction having diff magnitude. Vector x is called eigen vector and length is called eigen value. There can be multiple eigen vectors which are always orthogonal to each other."],"metadata":{"id":"TdUR0d3cXXP0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"cFCASQbu5AB-"},"outputs":[],"source":["# Creating 3D dataset\n","\n","import numpy as np\n","from scipy.spatial.transform import Rotation\n","\n","m = 60\n","X = np.zeros((m, 3))  # initialize 3D dataset\n","np.random.seed(42)\n","angles = (np.random.rand(m) ** 3 + 0.5) * 2 * np.pi  # uneven distribution\n","X[:, 0], X[:, 1] = np.cos(angles), np.sin(angles) * 0.5  # oval\n","X += 0.28 * np.random.randn(m, 3)  # add more noise\n","X = Rotation.from_rotvec([np.pi / 29, -np.pi / 20, np.pi / 4]).apply(X)\n","X += [0.2, 0, 0.2]  # shift a bit"]},{"cell_type":"code","source":["import numpy as np\n","\n","# X = [...]  # the small 3D dataset was created earlier in this notebook\n","X_centered = X - X.mean(axis=0)\n","U, s, Vt = np.linalg.svd(X_centered)\n","c1 = Vt[0]\n","c2 = Vt[1]"],"metadata":{"id":"OCgtx3D_Ko3e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["W2 = Vt[:2].T\n","X2D = X_centered @ W2"],"metadata":{"id":"VsbaV50zKyMs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","\n","pca = PCA(n_components=2)\n","X2D = pca.fit_transform(X)"],"metadata":{"id":"BIJm3CvjLNtz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pca.components_"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vQZJL2-FLR9k","executionInfo":{"status":"ok","timestamp":1709131658265,"user_tz":-330,"elapsed":36,"user":{"displayName":"Saipriya M Ravi","userId":"01294885343977700205"}},"outputId":"943e73b4-f912-42b6-f6cb-e2d2a17d2751"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.67857588,  0.70073508,  0.22023881],\n","       [ 0.72817329, -0.6811147 , -0.07646185]])"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["pca.explained_variance_ratio_"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q_OZTsvZLUPd","executionInfo":{"status":"ok","timestamp":1709131658266,"user_tz":-330,"elapsed":35,"user":{"displayName":"Saipriya M Ravi","userId":"01294885343977700205"}},"outputId":"72317c99-83e4-41c7-ae53-f61da6a8fd9a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.7578477 , 0.15186921])"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["1 - pca.explained_variance_ratio_.sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DK38UWqYLW5R","executionInfo":{"status":"ok","timestamp":1709131658266,"user_tz":-330,"elapsed":33,"user":{"displayName":"Saipriya M Ravi","userId":"01294885343977700205"}},"outputId":"39f4d690-c9b6-4b21-8f90-9f91fe3588dc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.09028309326742034"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["from sklearn.datasets import fetch_openml\n","\n","mnist = fetch_openml('mnist_784', as_frame=False, parser=\"auto\")\n","X_train, y_train = mnist.data[:60_000], mnist.target[:60_000]\n","X_test, y_test = mnist.data[60_000:], mnist.target[60_000:]\n","\n","pca = PCA(n_components=0.95)\n","X_reduced = pca.fit_transform(X_train)"],"metadata":{"id":"yCxfJf_oLe6j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pca.n_components_"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x7bTD-1mLg-G","executionInfo":{"status":"ok","timestamp":1709131787410,"user_tz":-330,"elapsed":580,"user":{"displayName":"Saipriya M Ravi","userId":"01294885343977700205"}},"outputId":"f2acc2bc-96dd-4952-e03d-fbf531f0e477"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["154"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["pca.explained_variance_ratio_.sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_dxu7p2rLhnW","executionInfo":{"status":"ok","timestamp":1709131787869,"user_tz":-330,"elapsed":6,"user":{"displayName":"Saipriya M Ravi","userId":"01294885343977700205"}},"outputId":"d546a168-2b30-4280-b4cd-2d0ad679e4a8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9501960192613035"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["pca = PCA(0.95)\n","X_reduced = pca.fit_transform(X_train, y_train)"],"metadata":{"id":"EgCKphT5Lokp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_recovered = pca.inverse_transform(X_reduced)"],"metadata":{"id":"l76vyfBfLpHQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rnd_pca = PCA(n_components=154, svd_solver=\"randomized\", random_state=42)\n","X_reduced = rnd_pca.fit_transform(X_train)"],"metadata":{"id":"gvapYuhwLuQ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","from sklearn.decomposition import IncrementalPCA\n","\n","n_batches = 100\n","inc_pca = IncrementalPCA(n_components=154)\n","for X_batch in np.array_split(X_train, n_batches):\n","    inc_pca.partial_fit(X_batch)\n","\n","X_reduced = inc_pca.transform(X_train)"],"metadata":{"id":"lORy1QbgLu6f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Random Projection\n","\n","Projects the data to a lower dimensional space using a random linear projection. It preserves distances. So 2 similar instances will remain similar and 2 very different instances will remain very different. Generates a random matrix P of shape [d,n] where each item is sampled randomly from a Gaussian distribution with mean 0 and variance 1/d and use it to project a dataset from n dimensions down to d."],"metadata":{"id":"iQNP__MlpH4x"}},{"cell_type":"markdown","source":["# Locally Linear Embedding (LLE)\n","\n","-> Its a manifold learning technique. A nonlinear dimensionality reduction (NLDR) technique.\n","\n","-> LLE works by first measuring how each training instance linearly relates to its nearest neighbors and then looking for a low dimensional representation of the training set where these local relationships are best preserved."],"metadata":{"id":"KkihSLTZpKzZ"}}]}