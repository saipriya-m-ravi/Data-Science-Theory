{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOHMZvmIW2rui1854Di5P6K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Gradient:\n","\n","-> Gradient means change in value of a quantity with change in a given variable.\n","Gradient means slope that means inclination of line.\n","\n","slope=tan(theta)=(y2-y1)/(x2-x1) = d (f(x))/dx\n","\n","The gradient of a curve at any point is equal to the gradient of its tangent at that point on the curve.\n","\n","The gradient of two parallel lines is equal. m1=m2\n","\n","The product of the gradients of two perpendicular lines is -1 => m1.m2=âˆ’1\n","\n","-> The gradient of a scalar-valued multivariable function f(x,y,..), denoted del(f) - Collection of partial derivatives in the form of vector. This means its a vector-valued function. Gradient vector,\n","\n","del(f) =[ do(f)/do(x)\n","\n","          do(f)/do(y)\n","\n","          ......\n","        ]\n","\n","If you imagine standing at a point (x0,y0,..) in the input space of f, the vector del(f(x0,y0,..)) tells you which direction you should travel to increase the value of f most rapidly."],"metadata":{"id":"TW5TZ1GZE7VL"}},{"cell_type":"markdown","source":["## Gradient Descent\n","\n","-> Optimization algorithm. Basic idea is to tweak parameters iteratively in order to minimize a cost function.\n","\n","-> It start by filling theta with random values (random initialization). It computes the gradient of cost function with regard to each model parameter theta_j and it goes in the direction of descending gradient. Once gradient is zero, we've reached the minimum.\n","\n","-> We need to calculate how much the cost function will change if we change theta_j just a little bit. Its called partial derivative.\n","\n","-> Size of steps is determined by learning rate hyperparameter. Learning step size is proportional to the slope of cost function so the steps gradually gets smaller as the cost approaches the minimum. If learning rate is too small , algo has to go through many iterations to converge which will take long time. If learning rate is too high algorithm diverges with larger and larger values failing to find a good solution.\n","\n","-> Convergence will become difficult if cost function graph has holes,ridges,plateaus (basically irregular terrain). Sometimes it will converge to a local minimum. Sometimes it take very long time to cross plateau and if we stop early, we will never reach global minimum.\n","\n","-> MSE cost function for linear regression is a convex function. That means if we pick any 2 points on the curve, the line segment joining them is never below the curve. That means there's no local minima and just one global minimum. Its a continous function with a slope that never changes abruptly. So gradient descent is guranteed to approach global minimum.\n","\n","-> When using gradient descent all features should have similar scale. Otherwise it will take much longer to converge.\n","\n","-> Training a model means seraching for a combination of model parameters that minimizes the cost function. Its a search in the model's parameter space."],"metadata":{"id":"iE_SP00ldmIk"}},{"cell_type":"markdown","source":["Computational complexity of training a linear regression model using normal equation or the SVD approach is linear with regard to both the number of instances we want to make predictions on and the number of features. Gradient descent scales well with the number of features.\n","\n","theta(next step)=theta-eta*del(MSE(theta))"],"metadata":{"id":"hVZcVfPESETx"}},{"cell_type":"markdown","source":["## 1] Batch gradient descent\n","\n","Use entire training set to compute gradient in every iteration. Its terribly slow on large training sets. Here we gently decrease cost function at each iteration until it reaches minima. Batch GD with a fixed learning rate will eventually converge to optimal solution.\n","\n","## 2] Stochastic gradient descent\n","\n","It randomly picks one instance at every step and compute gradient based on that. Here cost function will bounce up and down decreasing only average. Over time it will end up close to minima but even after that it will bounce around, never settling down. When algorithm stops, final parameter values will be good but not optimal.\n","\n","When cost function is irregular, it helps to jump out of local minima so SGD has better chance of finding global minima than batch GD. But algo never settle at minima. SOlution - Start with large learning rate - quick progress and escape local minima - then get smaller and smaller to settle at global minimum. Function that determines learning rate at each iteration is called the learning schedule.\n","\n","Learning rate reduced quickly - may get stucjk in local minima\n","\n","Learning rate reduced slowly - May jump around the minimum for long time and end up in a suboptimal solution if we halt training too early.\n","\n","## 3] Mini-Batch gradient descent\n","\n","Computes gradients on small random sets of instances called mini batches. Advantage over SGD - performance boost from hardware optimization of matrix operations.\n","\n","Batch GD actually stops at the minimum, while other two continue to walk around."],"metadata":{"id":"yrQ4kgNkXTm7"}},{"cell_type":"code","source":["## Generating some linear-looking data\n","import numpy as np\n","np.random.seed(42)\n","m=100\n","X=2*np.random.rand(m,1)\n","y=4+3*X+np.random.randn(m,1)  ## last term refers to noise"],"metadata":{"id":"LIJNug0rUbFf","executionInfo":{"status":"ok","timestamp":1708597222839,"user_tz":-330,"elapsed":443,"user":{"displayName":"Saipriya M Ravi","userId":"01294885343977700205"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"id":"VXzbmMA1Dq0T","executionInfo":{"status":"ok","timestamp":1708597267586,"user_tz":-330,"elapsed":413,"user":{"displayName":"Saipriya M Ravi","userId":"01294885343977700205"}}},"outputs":[],"source":["from sklearn.preprocessing import add_dummy_feature\n","X_b=add_dummy_feature(X)"]},{"cell_type":"code","source":["## Batch GD\n","eta=0.1\n","n_epochs=1000\n","m=len(X_b)\n","np.random.seed(42)\n","theta=np.random.randn(2,1)\n","theta"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e4k5IPeXUjrF","executionInfo":{"status":"ok","timestamp":1708597223288,"user_tz":-330,"elapsed":11,"user":{"displayName":"Saipriya M Ravi","userId":"01294885343977700205"}},"outputId":"ac1131cb-e8de-4d4e-832b-2256bc91c1f1"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.49671415],\n","       [-0.1382643 ]])"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["for epoch in range(n_epochs):\n","  gradients=2/m*X_b.T@(X_b@theta - y)\n","  theta=theta-eta*gradients"],"metadata":{"id":"troCAd6lU2Nb","executionInfo":{"status":"ok","timestamp":1708597275235,"user_tz":-330,"elapsed":421,"user":{"displayName":"Saipriya M Ravi","userId":"01294885343977700205"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["-> We can use Grid search to find optimal learning rate.\n","\n","-> How to set the number of epochs?\n","\n","Too low => We will be far away from the optimal solution when the algorithm stops.\n","\n","Too high => Waste of time when the model parameters do not change anymore\n","\n","Solution => Set large number of epochsbut to interrupt algorithm when the gradient vector becomes tiny that means norm becomes smaller than a tiny number (tolerance). Beacuse this happens when gradient descent has almost reached the minimum."],"metadata":{"id":"7C4p6q7TV8V0"}},{"cell_type":"code","source":["## SGD\n","m=len(X_b)\n","n_epochs=50\n","t0,t1=5,50\n","\n","def learning_schedule(t):\n","  return t0/(t+t1)\n","\n","np.random.seed(42)\n","theta=np.random.randn(2,1)\n","\n","for epoch in range(n_epochs):\n","  for iteration in range(m):\n","    random_index=np.random.randint(m)\n","    xi=X_b[random_index:random_index+1]\n","    yi=y[random_index:random_index+1]\n","    gradients=2*xi.T@(xi@theta-yi)   ## For SGD do not divide by m\n","    eta=learning_schedule(epoch*m+iteration)\n","    theta=theta-eta*gradients\n","theta"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HMAYiGwbVWRW","executionInfo":{"status":"ok","timestamp":1708599823051,"user_tz":-330,"elapsed":381,"user":{"displayName":"Saipriya M Ravi","userId":"01294885343977700205"}},"outputId":"44d6daa4-2048-4f14-8ec1-29c6183ebcdf"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[4.21076011],\n","       [2.74856079]])"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":[],"metadata":{"id":"IWZmTC_rfPDw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Linear regression using stochastic GD with Scikit-Learn - SGDRegressor class defaults to optimizing MSE cost function\n","from sklearn.linear_model import SGDRegressor\n","sgd_reg=SGDRegressor(max_iter=1000,tol=1e-5,penalty=None,eta0=0.01,n_iter_no_change=100,random_state=42)\n","sgd_reg.fit(X,y.ravel())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":75},"id":"MrupFs-IfAiw","executionInfo":{"status":"ok","timestamp":1708600013752,"user_tz":-330,"elapsed":394,"user":{"displayName":"Saipriya M Ravi","userId":"01294885343977700205"}},"outputId":"24edf632-dcc5-4131-f062-8c37ca14bd1e"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SGDRegressor(n_iter_no_change=100, penalty=None, random_state=42, tol=1e-05)"],"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDRegressor(n_iter_no_change=100, penalty=None, random_state=42, tol=1e-05)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDRegressor</label><div class=\"sk-toggleable__content\"><pre>SGDRegressor(n_iter_no_change=100, penalty=None, random_state=42, tol=1e-05)</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["sgd_reg.intercept_,sgd_reg.coef_"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aqeNcW8Bfy_Q","executionInfo":{"status":"ok","timestamp":1708600035894,"user_tz":-330,"elapsed":378,"user":{"displayName":"Saipriya M Ravi","userId":"01294885343977700205"}},"outputId":"4c404637-745e-4242-93ee-e7a933e25fcd"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([4.21278812]), array([2.77270267]))"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":[],"metadata":{"id":"zg3Ii0Zwf4O8"},"execution_count":null,"outputs":[]}]}