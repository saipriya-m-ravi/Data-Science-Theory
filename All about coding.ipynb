{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOHzX7udiG0i4OmWx9u1bqk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Libraries"],"metadata":{"id":"KzaWbmI8VIQz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5WOY7qDyYr_z"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from collections import defaultdict\n","\n","## Visualisation libraries\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","## Hypothesis testing\n","from scipy.stats import (ttest_ind, f_oneway, shapiro, levene, chi2_contingency)\n","from statsmodels.stats.proportion import proportions_ztest # For proportion z test\n","from statsmodels.stats.anova import anova_lm # For n-way anova\n","from statsmodels.formula.api import ols # For n-way anova\n","from scipy import stats\n","import statsmodels.api as sm ## For QQ plot\n","\n","## Modelling\n","from sklearn.model_selection import (train_test_split,KFold,cross_val_score,GridSearchCV)\n","from sklearn.preprocessing import (StandardScaler,MinMaxScaler)\n","from sklearn.metrics import (mean_squared_error,r2_score,mean_absolute_error,mean_absolute_percentage_error\n","                             accuracy_score,confusion_matrix,\n","                             roc_auc_score,roc_curve,auc,\n","                             precision_recall_curve,classification_report)\n","from sklearn.impute import KNNImputer\n","from sklearn.linear_model import (Lasso,Ridge,LinearRegression,LogisticRegression)\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","from sklearn.utils import class_weight\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","import xgboost as xgb\n","from imblearn.over_sampling import SMOTE\n","\n","import scipy.cluster.hierarchy as sch\n","from sklearn.cluster import KMeans\n","from scipy.stats import pearsonr\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.neighbors import NearestNeighbors\n","from cmfrec import CMF\n","from statsmodels.tsa.stattools import adfuller\n","from statsmodels.tsa.seasonal import seasonal_decompose\n","from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n","from statsmodels.tsa.arima.model import ARIMA\n","\n","##Generic\n","import re\n","import os\n","import random\n","import time\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","source":["# Reading data and creating dataframe"],"metadata":{"id":"93MfWRmAVn-u"}},{"cell_type":"code","source":["df=pd.read_csv(\"netflix.csv\")\n","\n","import gdown\n","url='https://drive.google.com/file/d/1gHYYLqLt6rMyeAyvHf1wvlQ4BLKwjv9W/view?usp=sharing'\n","ider=url.split('/')[-2]\n","!gdown --id $ider\n","train = pd.read_csv('/content/train_1.csv')"],"metadata":{"id":"eELZY9B0Vs3O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Examining dataset"],"metadata":{"id":"7lHMCeERVwIV"}},{"cell_type":"code","source":["df.head()\n","df.shape\n","df.dtypes\n","df.info()\n","df.columns\n","df.select_dtypes('object').columns\n","df.select_dtypes(np.number).columns\n","num_cols=data.select_dtypes(include=np.number).columns\n","\n","df.isnull().sum()/len(df)*100\n","df.isna().sum()\n","\n","df['Miles'].min(), df['Miles'].max()\n","\n","df.sort_values(by=\"release_year\").head(20)\n","df_new=new_train.sort_index( ascending=[True,True])  ## If there are multiple indexes\n","\n","df[\"director\"].value_counts()[:10]\n","data['zip_code'].value_counts(normalize=True)*100\n","df['company_hash'].value_counts().sort_index()\n","\n"," ## To get descriptive statistics\n","df['Income'].describe()\n","df.describe()\n","df.describe(include='all').T"],"metadata":{"id":"qGzRUdlbV86A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exploratory data analysis"],"metadata":{"id":"u2WNNtwvWSz-"}},{"cell_type":"code","source":["##Date columns\n","data['MMM-YY']=pd.to_datetime(data['MMM-YY'])\n","\n","## Column operations\n","movies.rename(columns={'Movie ID':'MovieID'}, inplace=True)\n","movies1=movies.copy()\n","\n","data['Rating']=data['Rating'].astype('int32')\n","\n","## Creating years of experience column\n","df['years_of_experience']=2024-df['orgyear']\n","\n","## Unique Values\n","df['rating'].unique()   # <=To get list of unique values\n","print(df.title.nunique() == df.shape[0])\n","\n","## Missing values\n","df['date_added'].isna().sum()\n","df[df['date_added'].isna()]\n","##Sometimes we need to extract year from date . More than exact date, growth over years is significant.\n","\n","## Filling null values\n","df['orgyear'].fillna(df.groupby('company_hash')['orgyear'].transform('median'), inplace=True)\n","df['job_position'] = df['job_position'].fillna('Others')\n","\n","df = df.loc[~df['orgyear'].isna()]\n","\n","## Dropping null values\n","df.dropna(inplace=True)\n","\n","## Filtering\n","country_list=[\"United States\",\"India\",\"United kingdom\",\"Canada\",\"Japan\"]\n","df2=df1[df1[\"country\"].isin(country_list)]\n","\n","## Working with data\n","x=df['cast'].apply(lambda x: str(x).split(', ')).tolist()\n","df_new=pd.DataFrame(x,index=df['title'])\n","\n","df.reset_index(inplace=True)\n","df.set_index('index', inplace=True)\n","\n","def preprocess_string(string):\n","    new_string= re.sub('[^A-Za-z ]+', '', string).lower().strip()\n","    return new_string\n","\n","df.job_position=df.job_position.apply(lambda x: preprocess_string(str(x)))\n","\n","## Grouping\n","df.groupby('Gender')['User_ID'].nunique()\n","df.groupby('Gender')['Purchase'].describe()\n","df.sample(300).groupby('Gender')['Purchase'].describe()\n","data.groupby(by=['grade','loan_status']).size()\n","df.groupby(['region','sex','smoker'])['hospitalization charges'].mean().unstack()\n","\n","function_dict = {'Age':'max', 'Gender':'first','City':'first',\n"," 'Education_Level':'last', 'Income':'last'}\n","new_train=df.groupby(['Driver_ID','MMM-YY']).aggregate(function_dict)\n","new_train.head(10)\n","\n","df1['Gender'] = list(df_new.groupby('Driver_ID').agg({'Gender':'last'})['Gender'])\n","\n","## Dropping duplicates,columns\n","df.drop_duplicates(inplace=True)\n","df.drop(columns='cast_x', inplace=True)\n","df.drop([\"Unnamed: 0\"],axis=1,inplace=True)\n","\n","## Outlier treatment\n","percentiles = df[col].quantile([0.05,0.95]).values\n","df[col] = np.clip(df[col], percentiles[0], percentiles[1])\n","\n","df['ctc'] = df['ctc'].clip(lower=df.ctc.quantile(0.01), upper=df.ctc.quantile(0.99))\n","\n","q1=df['Income'].quantile(0.25)\n","q3=df['Income'].quantile(0.75)\n","iqr=q3-q1\n","df=df.loc[(df['Income']>(q1-1.5*iqr)) & (df['Income']<(q3+1.5*iqr))]\n","\n","## Binning data\n","bins = [-1,20,25,30,35,40,55]\n","labels = ['<20','20-25','25-30','30-35','35-40','40+']\n","df['Age_bins'] = pd.cut(df['Age'], bins=bins, labels=labels)\n","\n","## Merging\n","final=df.merge(df_new,on='title',how='inner')\n","\n","## Combining minority classes to a single one\n","data.loc[(data[\"home_ownership\"]==\"ANY\") | (data[\"home_ownership\"]==\"NONE\"),\"home_ownership\"]=\"OTHER\"\n","\n","## Creating a balanced dataset\n","df_new['workingday'].value_counts()\n","working=df_new[df_new['workingday']==1]['count'].sample(3425) ## 3425 is the minimum value in the value_counts output\n","nonworking=df_new[df_new['workingday']==0]['count'].sample(3425)\n","\n","## Mapping of target variable\n","data['loan_status']=data['loan_status'].map({'Fully Paid':0,'Charged Off':1})\n","\n","## Applying function\n","def fun(number):\n","  if number==0.0:\n","    return 0\n","  else:\n","    return 1\n","\n","data['pub_rec']=data['pub_rec'].apply(fun)\n","\n","data['zip_code']=data['address'].apply(lambda x:x[-5:]) ## Extracting zip code from address\n","\n","## Correlation between columns. First convert categorical to numerical\n","df['Gender'].replace(['Male', 'Female'], [1, 0], inplace=True)\n","df.corr()\n","\n","## For probabilities (conditional,marginal)\n","pd.crosstab(index=df['Gender'],columns=df['Product'])\n","pd.crosstab(index=df['Gender'],columns=df['Product'],margins=True)\n","pd.crosstab(index=df['Gender'],columns=df['Product'],margins=True,normalize=True)*100\n","pd.crosstab(index=df['Gender'],columns=df['Product'],margins=True,normalize='index')\n","\n","## Mean target imputation\n","total_acc_avg=data.groupby(by='total_acc').mean(numeric_only=True)['mort_acc']\n","\n","def fill_mort_acc(total_acc,mort_acc):\n","  if np.isnan(mort_acc):\n","    return total_acc_avg[total_acc].round()\n","  else:\n","    return mort_acc\n","\n","data['mort_acc']=data.apply(lambda x: fill_mort_acc(x['total_acc'],x['mort_acc']),axis=1)\n","\n","##KNN imputation\n","imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n","imputer.fit(data_nums)\n","data_new=imputer.transform(data_nums)\n","data_new=pd.DataFrame(data_new)\n","data_new.columns=cols\n","data_new.head()\n","\n","## One hot encoding\n","dummies=['purpose','zip_code','home_ownership','grade','verification_status','application_type']\n","data=pd.get_dummies(data,columns=dummies,drop_first=True)\n","\n","df1 = pd.concat([df1,pd.get_dummies(df1['City'],prefix='City')],axis=1)\n","df1.head()\n","\n","##Generic\n","rem_cols=list(set(data.columns).difference(set(cols)))"],"metadata":{"id":"0qtfW7QjWX4n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visual analysis"],"metadata":{"id":"HaVzweHDZDvb"}},{"cell_type":"code","source":["df['type'].value_counts().plot(kind='pie')\n","plt.pie(df[\"Product\"].value_counts(),labels=[\"KP281\",\"KP481\",\"KP781\"],autopct=\"%0f%%\")\n","plt.show()\n","\n","## Plotting correlation\n","sns.heatmap(df.corr(), cmap=\"YlGnBu\", annot=True)\n","sns.heatmap(data.corr(numeric_only=True),annot=True,cmap='Greens')\n","\n","## For identifying outliers\n","sns.boxplot(x=df[\"Age\"])\n","\n","plt.figure(figsize=(15,6))\n","for i,j in enumerate(categorical):\n","  plt.subplot(1,4,i+1)\n","  plt.subplots_adjust(hspace=0.8)\n","  sns.boxplot(x=j,y='count',data=df)\n","  plt.tight_layout(pad=1)\n","\n","## Multivariate analysis using box plot\n","sns.boxplot(x=\"Product\", y=\"Usage\", hue=\"Gender\", data=df)\n","x=df[df['Gender']==\"Male\"]\n","print(x[x[\"Product\"]==\"KP481\"][\"Usage\"].value_counts())\n","\n","## Count plot\n","sns.countplot(x=\"Product\", hue=\"MaritalStatus\", data=df)\n","\n","## Violin plot\n","sns.violinplot(x=\"Fitness\", y=\"Age\", data=df)\n","\n","## Line plot\n","sns.lineplot(x='age',y='hospitalization charges',data=df,hue='sex')\n","\n","##Bar plot\n","sns.barplot(x='age_bins',y='hospitalization charges',data=df)\n","\n","plt.subplot(231)\n","df1['Grade'].value_counts(normalize=True).plot.bar(title='Grade')\n","\n","## To observe association between 2 variables\n","plt.scatter(df['Age'], df['Income'],linewidths = 2,marker =\"o\",edgecolor =\"green\",s = 50)\n","sns.scatterplot(x=\"CGPA\", y=\"LOR \", data=df, hue=\"Research\")\n","\n","sns.regplot(x=\"GRE Score\", y=\"TOEFL Score\", data=df)\n","\n","## Analysis of categorical columns\n","sns.distplot(df['Age'], hist=True, kde=True, bins=int(36))  ## Binning age and plotting distribution\n","\n","sns.displot(x='Purchase',data=df,bins=25,hue='Gender')\n","plt.xlabel('Purchase')\n","plt.ylabel('Frequency')\n","plt.title('Histogram of purchase')\n","\n","sns.histplot(data=df,x=\"release_year\",hue=\"type\")"],"metadata":{"id":"tYAMrp60ZFq4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Hypothesis Testing"],"metadata":{"id":"N3XAA6ySYigN"}},{"cell_type":"code","source":["## T-test independent\n","t_statistic,p_value=ttest_ind(working,nonworking,equal_var=False,alternative='greater')\n","\n","## Shapiro test\n","statistics,p_value=shapiro(df)\n","\n","## Levene test\n","statistics,p_value=levene(df1,df2,df3)\n","\n","## One way ANOVA\n","t_stats,p_value=f_oneway(df1,df2,df3)\n","\n","## Contigency table\n","contigency=pd.crosstab(df_new['season'],df_new['weather'])\n","contigency.plot(kind='bar')\n","#chi2 contigency test\n","chi2,pval,dof,exp_freq=chi2_contingency(contigency,correction=False)\n","\n","## QQ plot check\n","import statsmodels.api as sm\n","df_std=(df_female_severe['viral load']-df_female_severe['viral load'].mean())\n","sm.qqplot(df_std,line='45')\n","plt.show()\n"],"metadata":{"id":"MiNtsuUtYlXe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Supervised Learning"],"metadata":{"id":"HdudMnAPqvo5"}},{"cell_type":"code","source":["## Train test split\n","X = df.drop(['Chance of Admit '], axis=1)\n","y = df['Chance of Admit ']\n","X_train, X_test, y_train, y_test = train_test_split(X,y,test_size =0.20, shuffle=True)\n","X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.30,stratify=y,random_state=42)\n","\n","##Scaling\n","std=StandardScaler()\n","X_train_std=std.fit_transform(X_train)\n","\n","scaler=MinMaxScaler()\n","X_train=scaler.fit_transform(X_train)\n","X_test=scaler.fit_transform(X_test)\n","\n","## Multiple model results - Linear,Lasso,Ridge\n","models = [\n","['Linear Regression :', LinearRegression()],\n","['Lasso Regression :', Lasso(alpha=0.1)],\n","['Ridge Regression :', Ridge(alpha=1.0)]\n","]\n","\n","for name,model in models:\n","  model.fit(X_train, y_train.values)\n","  predictions = model.predict(std.transform(X_test))\n","  print (name, (np.sqrt(mean_squared_error(y_test, predictions))))\n","  print ('Mean Absolute Error ', mean_absolute_error(y_test.values,pred) )\n","\n","## Linear Regression using statsmodels\n","import statsmodels.api as sm\n","X_train = sm.add_constant(X_train)\n","model = sm.OLS(y_train.values, X_train).fit()\n","print (model.summary())\n","\n","## Linear regression assumptions check\n","## VIF - Checking Multicollinearity\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","def calculate_vif(dataset,col):\n","  dataset=dataset.drop(columns=col,axis=1)\n","  vif=pd.DataFrame()\n","  vif['features']=dataset.columns\n","  vif['VIF_Value']=[variance_inflation_factor(dataset.values,i) for i in range(len(dataset.columns))]\n","  return vif\n","\n","calculate_vif(X_train_new,[])\n","\n","## Mean of Residuals\n","residuals = y_test.values-pred\n","mean_residuals = np.mean(residuals)\n","print (\"Mean of Residuals {}\".format(mean_residuals))\n","\n","## Test for Homoscedasticity\n","p = sns.scatterplot(x=pred,y=residuals)\n","plt.xlabel('predicted values')\n","plt.ylabel('Residuals')\n","plt.ylim(-0.4,0.4)\n","plt.xlim(0,1)\n","p = plt.title('Residuals vs fitted values plot for homoscedasticity check')\n","\n","import statsmodels.stats.api as sms\n","from statsmodels.compat import lzip\n","name = ['F statistic','p-value']\n","test = sms.het_goldfeldquandt(residuals, X_test)\n","lzip(name, test)\n","\n","## Normality of residuals\n","p = sns.distplot(residuals,kde=True)\n","p = plt.title('Normality of error terms/residuals')\n","\n","## Logistic Regression\n","logreg=LogisticRegression(max_iter=1000)\n","logreg.fit(X_train,y_train)\n","y_pred=logreg.predict(X_test)\n","\n","## Performance evaluation - Confusion Matrix,Classification report,ROC-AUC Curve,Precision Recall curve, Cross validation\n","confusion_matrix=confusion_matrix(y_test,y_pred)\n","print(confusion_matrix)\n","\n","print(classification_report(y_test,y_pred))\n","\n","logit_roc_auc=roc_auc_score(y_test,logreg.predict(X_test))\n","fpr,tpr,thresholds=roc_curve(y_test,logreg.predict_proba(X_test)[:,1])\n","plt.figure()\n","plt.plot(fpr,tpr,label='Logistic Regression (area = %0.2f)'%logit_roc_auc)\n","plt.plot([0,1], [0,1],'r--')\n","plt.xlim([0.0,1.0])\n","plt.ylim([0.0,1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver operating characteristic')\n","plt.legend(loc=\"lower right\")\n","plt.savefig('Log_ROC')\n","plt.show()\n","\n","def precision_recall_curve_plot(y_test,pred_proba_c1):\n","  precisions,recalls,thresholds=precision_recall_curve(y_test,pred_proba_c1)\n","  threshold_boundary=thresholds.shape[0]\n","  # plot precision\n","  plt.plot(thresholds,precisions[0:threshold_boundary],linestyle='--',label='precision')\n","  # plot recall\n","  plt.plot(thresholds,recalls[0:threshold_boundary],label='recalls')\n","  start,end=plt.xlim()\n","  plt.xticks(np.round(np.arange(start,end,0.1),2))\n","  plt.xlabel('Threshold Value');\n","  plt.ylabel('Precision and Recall Value')\n","  plt.legend();\n","  plt.grid()\n","  plt.show()\n","\n","precision_recall_curve_plot(y_test,logreg.predict_proba(X_test)[:,1])\n","\n","X=scaler.fit_transform(X)\n","kfold=KFold(n_splits=5)\n","accuracy=np.mean(cross_val_score(logreg,X,y,cv=kfold,scoring='accuracy'))\n","print(\"Cross Validation accuracy: {:.3f}\".format(accuracy))\n","\n","##Random Forest Classifier\n","param = {'max_depth':[2,3,4], 'n_estimators':[50,100,150,200]}\n","random_forest = RandomForestClassifier(class_weight ='balanced')\n","c = GridSearchCV(random_forest,param,cv=3,scoring='f1')\n","c.fit(X_train,y_train)\n","def display(results):\n"," print(f'Best parameters are : {results.best_params_}')\n"," print(f'The score is : {results.best_score_}')\n","display(c)\n","y_pred = c.predict(X_test)\n","\n","print(classification_report(y_test, y_pred))\n","cm = confusion_matrix(y_test, y_pred)\n","print(cm)\n","\n","## Feature importances\n","importances = random_forest.feature_importances_\n","std = np.std([tree.feature_importances_ for tree in random_forest.estimators_], axis=0)\n","\n","## Balancing Using SMOTE\n","print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1)))\n","print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0)))\n","\n","sm = SMOTE(random_state = 7)\n","X_train, y_train = sm.fit_resample(X_train, y_train.ravel())\n","\n","print('After OverSampling, the shape of train_X: {}'.format(X_train.shape))\n","print('After OverSampling, the shape of train_y: {} \\n'.format(y_train.shape))\n","\n","print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train == 1)))\n","print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train == 0)))\n","\n","## XGBoost classifier\n","\n","my_model = xgb.XGBClassifier(class_weight ='balanced')\n","my_model.fit(X_train, y_train)\n","\n","y_pred = my_model.predict(X_test)\n","\n","print(classification_report(y_test, y_pred))\n","cm = confusion_matrix(y_test, y_pred)\n","print(cm)\n","\n","## Decision Tree Classifier\n","clf = DecisionTreeClassifier()\n","clf = clf.fit(X_train,y_train)\n","y_pred = clf.predict(X_test)\n","\n","print(classification_report(y_test, y_pred))\n","cm = confusion_matrix(y_test, y_pred)\n","print(cm)"],"metadata":{"id":"V6IgEtPKqwCh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Unsupervised Learning"],"metadata":{"id":"djocjastWvBz"}},{"cell_type":"code","source":["## Manual clustering\n","grouped_c_j_y = df.groupby(['years_of_experience','job_position','company_hash'])['ctc'].describe()\n","df_cjy=df.merge(grouped_c_j_y, on=['years_of_experience','job_position','company_hash'], how='left')\n","df_cjy.drop(columns=['count','mean','std','min','25%','50%','75%','max'],inplace=True)\n","\n","## Hierarchical clustering\n","import scipy.cluster.hierarchy as sch\n","\n","sample = df.sample(500)\n","Z = sch.linkage(sample, method='ward')\n","\n","fig, ax = plt.subplots(figsize=(20, 12))\n","sch.dendrogram(Z, labels=sample.index, ax=ax, color_threshold=2)\n","plt.xticks(rotation=90)\n","ax.set_ylabel('distance')\n","\n","## K means clustering\n","k = 3\n","kmeans = KMeans(n_clusters=k, random_state=42)\n","y_pred = kmeans.fit_predict(X_sc)\n","\n","kmeans.cluster_centers_   ## coordinates of the cluster centers\n","clusters = pd.DataFrame(X_sc, columns=X.columns)\n","clusters['label'] = kmeans.labels_\n","\n","x_axis = 'years_of_experience'\n","y_axis = 'class'\n","\n","plt.scatter(clusters[x_axis], clusters[y_axis], c=clusters['label'], )\n","plt.scatter(kmeans.cluster_centers_[:, 1], kmeans.cluster_centers_[:, 2], color=\"red\", marker=\"X\", s=100)\n","plt.xlabel(x_axis)\n","plt.ylabel(y_axis)"],"metadata":{"id":"MoE2ozW-WyCd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Pivot table creation\n","matrix = pd.pivot_table(data, index='UserID', columns='Title', values='Rating', aggfunc='mean')\n","rm = data.pivot(index = 'UserID', columns ='MovieID', values = 'Rating').fillna(0)\n","\n","similar_movies = matrix.corrwith(movie_rating)\n","\n","item_sim = cosine_similarity(matrix.T)\n","item_sim_matrix = pd.DataFrame(item_sim, index=matrix.columns, columns=matrix.columns)\n","\n","## Nearest Neighbors\n","model_knn = NearestNeighbors(metric='cosine')\n","model_knn.fit(matrix.T)\n","distances, indices = model_knn.kneighbors(matrix.T, n_neighbors= 6)\n","result = pd.DataFrame(indices, columns=['Title1', 'Title2', 'Title3', 'Title4', 'Title5','Title6'])\n","\n","result2 = result.copy()\n","for i in range(1, 7):\n","    mov = pd.DataFrame(matrix.T.index).reset_index()\n","    mov = mov.rename(columns={'index':f'Title{i}'})\n","    result2 = pd.merge(result2, mov, on=[f'Title{i}'], how='left')\n","    result2 = result2.drop(f'Title{i}', axis=1)\n","    result2 = result2.rename(columns={'Title':f'Title{i}'})\n","result2.head()\n","\n","## Matrix Factorization\n","\n","## 1. Using cmfrec library\n","model = CMF(method=\"als\", k=4, lambda_=0.1, user_bias=False, item_bias=False, verbose=False)\n","model.fit(user_itm) #Fitting the model\n","model.A_.shape, model.B_.shape #model.A_ gives the embeddings of Users and model.B_ gives the embeddings of Items.\n","user_itm.Rating.mean(), model.glob_mean_  # Average rating and Global Mean\n","rm__ = np.dot(model.A_, model.B_.T) + model.glob_mean_ #Calculating the predicted ratings"],"metadata":{"id":"cqIbH-Ltnrz-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Dickey-Fuller test - For checking stationarity\n","\n","# Null Hypothesis (H0): Series is non-stationary\n","# Alternate Hypothesis (HA): Series is stationary\n","# p-value >0.05 Fail to reject (H0)\n","# p-value <= 0.05 Accept (H1)\n","\n","def df_test(x):\n","    result=adfuller(x)\n","    print('ADF Stastistic: %f'%result[0])\n","    print('p-value: %f'%result[1])\n","\n","df_test(total_view['en'])\n","\n","## Remove trend and seasonality with decomposition\n","decomposition = seasonal_decompose(ts.values, model='multiplicative',period=7)\n","trend = decomposition.trend\n","seasonal = decomposition.seasonal\n","residual = decomposition.resid\n","\n","plot.figure(figsize=(10,7))\n","plot.subplot(411)\n","plot.title('Observed = Trend + Seasonality + Residuals')\n","plot.plot(ts.values,label='Observed')\n","plot.legend(loc='best')\n","plot.subplot(412)\n","plot.plot(trend, label='Trend')\n","plot.legend(loc='best')\n","plot.subplot(413)\n","plot.plot(seasonal,label='Seasonality')\n","plot.legend(loc='best')\n","plot.subplot(414)\n","plot.plot(residual, label='Residuals')\n","plot.legend(loc='best')\n","plot.tight_layout()\n","plot.show()\n","\n","ts_decompose=pd.DataFrame(residual).fillna(0)[0].values\n","df_test(ts_decompose)\n","\n","## Remove trend and seasonality with differencing\n","ts_diff = ts - ts.shift(1)\n","plot.plot(ts_diff.values)\n","plot.show()\n","ts_diff.dropna(inplace=True)\n","df_test(ts_diff)\n","\n","## Plot the autocorrelation and partial auto correlation functions\n","acf=plot_acf(ts_diff,lags=20)\n","pacf=plot_pacf(ts_diff,lags=20)\n","\n","## ARIMA model\n","model = ARIMA(ts, order=(4,1,3))\n","model_fit = model.fit()\n","model_fit.predict(dynamic=False)\n","plot.show()\n","\n","## Multistep forecasting\n","train = ts[:-20]\n","test = ts[-20:]\n","model = ARIMA(train, order=(4, 1, 3))\n","fitted = model.fit()\n","fc=fitted.forecast(20, alpha=0.02)\n","fc_series = pd.Series(fc, index=test.index)\n","\n","plot.figure(figsize=(12,5), dpi=100)\n","plot.plot(train, label='training')\n","plot.plot(test, label='actual')\n","plot.plot(fc_series, label='forecast')\n","plot.title('Forecast vs Actuals')\n","plot.legend(loc='upper left', fontsize=8)\n","\n","mape = np.mean(np.abs(fc - test.values)/np.abs(test.values))\n","rmse = np.mean((fc - test.values)**2)**.5\n","\n","##SARIMAX\n","exog=ex_df['Exog'].to_numpy()\n","train=ts[:520]\n","test=ts[520:]\n","model=sm.tsa.statespace.SARIMAX(train,order=(4, 1, 3),seasonal_order=(1,1,1,7),exog=exog[:520])\n","results=model.fit()\n","\n","fc=results.forecast(30,dynamic=True,exog=pd.DataFrame(exog[520:]))"],"metadata":{"id":"3yhrZpUSrTf5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hI1_rVfdtahb"},"execution_count":null,"outputs":[]}]}