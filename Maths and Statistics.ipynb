{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNvwk7C3PQLtaUhGhxJmrWt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Statistics\n","\n","-> Whenever statisticians use data from a sample - subset of population - to make statements about a population, they are performing statistical inference.. Estimation and hypothesis testing are procedures used to make statistical inference. Probability is used to measure the quality and precision of inferences.\n","\n","-> Descriptive statistics are tabular, graphical and numerical summaries of data. Univariate methods use data to enhance understanding of a single variable. Multivariate methods are to understand relationships among 2 or more variables.\n","\n","-> Frequency distribution is the most common tabular summary of data.\n","\n","-> Graphical methods for categorical variables - bar and pie chart\n","\n","-> Graphical methods for numerical variables - histogram\n","\n","-> Numerical measures for qualitative data - proportion or percentage\n","\n","-> Numerical measures for quantitative data - Mean, median, mode, percentiles, range, variance, standard deviation\n","\n","Percentiles provide an indication of how the data values are spread over the interval from the smallest to largest values. p percent of data values fall below the pth percentile and 100-p percent of data values are above the pth percentile. Quartiles divide the data values into 4 parts- first quartile is the 25th percentile, second quartile is 50th percentile, third is 75th percentile.\n","\n","Range=largest-smallest. Measure of variability. Its determined only by 2 extreme values. Variance (s**2), standard deviation (s) are measures of variability based on all data values.\n","\n","Population variance, sigma^2 = sigma i=1 to N ((xi-mu)^2)/N\n","\n","Sample variance, s^2 = sigma i=1 to n ((xi-x_bar)^2) / (n-1)\n","\n","The reason dividing by n-1 corrects the bias is because we are using the sample mean, instead of the population mean, to calculate the variance. Since the sample mean is based on the data, it will get drawn toward the center of mass for the data.Using \"n - 1\" when calculating the variance for a sample helps to make sure that the number you get is a better representation of the whole population.\n","\n","-> Outliers - Data values that appear unusually large or small and out of place when compared with other data values.\n","\n","For normal distributions z score is used to remove outliers.Mean and standard deviation are used to identify outliers. A z-score can be computed for each data value. Z score represents the relative position of data value by indicating the number of standard deviations it is from mean. Any value with z score less than -3 or greater than +3 are considered to be outliers.\n","\n","z=(x-x_bar)/sigma\n","\n","For skewed distributions, use IQR for outlier removal\n","\n","For other distributions, use a percentile based approach. eg: data points far from 99 percentile and less than 1 percentile.\n","\n","-> EDA\n","\n","Five-number summary consists of smallest value, first quartile, median, third quartile and largest value. Box plot is a graphical representation of this. Whiskers will be extended to smallest/larget values and in case of outliers, whiskers will be extended to data values which are not outliers. Dots outside whiskers represents outliers.\n"],"metadata":{"id":"L4ocOJYVpYq0"}},{"cell_type":"markdown","source":["# Probability\n","\n","Probability theory is used to asses the risk associated with a particular decision. In finance to create a math model of stock market to predict future trends. In consumer industry, to reduce the probability of failure in product's design.\n","\n","**-> Random Experiment**\n","\n","An experiment is said to be a random experiment if there is more than one possible outcome, and it is impossible to predict the outcome in advance. All possible results of an experiment are called its outcomes.\n","\n","**-> Probability**\n","\n","The likelihood of occurrence of an event is known as probability. The probability of occurrence of any event lies between 0 and 1.\n","\n","Empirical probability is based on a ratio of the number of attempts of a task to the number of a specific result (e.g., coin tosses to number of heads or tails achieved). Experimental or empirical probability is the probability of an event based on the results of an actual experiment conducted several times.\n","\n","Theoretical probability starts with the desired outcome (heads) and relates it to the number of possible results (heads or tails). In theoretical probability, we assume that the probability of occurrence of any event is equally likely (outcomes have an equal chance of occurring) and based on that we predict the probability of an event.\n","\n","**-> Sample space**\n","\n","The entire possible set of outcomes of a random experiment\n","\n","**-> Event**\n","\n","A set of outcomes of an experiment or the subset of the respective sample space.\n","\n","The sample space for the tossing of three coins simultaneously is given by:\n","\n","S = {(T , T , T) , (T , T , H) , (T , H , T) , (T , H , H ) , (H , T , T ) , (H , T , H) , (H , H, T) ,(H , H , H)}\n","\n","Suppose, if we want to find only the outcomes which have at least two heads; then the set of all such possibilities can be given as:\n","\n","E = { (H , T , H) , (H , H ,T) , (H , H ,H) , (T , H , H)}\n","\n","Thus, an event is a subset of the sample space, i.e., E is a subset of S.\n","\n","**-> Impossible and Sure Events**\n","\n","If the probability of occurrence of an event is 0, such an event is called an impossible event and if the probability of occurrence of an event is 1, it is called a sure event. In other words, the empty set ϕ is an impossible event and the sample space S is a sure event.\n","\n","**-> Simple Events**\n","\n","Any event consisting of a single point of the sample space is known as a simple event in probability. For example, if S = {56 , 78 , 96 , 54 , 89} and E = {78} then E is a simple event.\n","\n","**-> Compound Events**\n","\n","if any event consists of more than one single point of the sample space then such an event is called a compound event. Considering the same example again, if S = {56 ,78 ,96 ,54 ,89}, E1 = {56 ,54 }, E2 = {78 ,56 ,89 } then, E1 and E2 represent two compound events.\n","\n","**-> Dependent events**\n","\n","Two events are said to be dependent if the occurrence of one event changes the probability of another event. eg: Not paying your power bill on time and having your power cut off.\n","\n","**-> Independent events**\n","\n","Occurrence of one event doesn't affect the probability of occurrence of the other event.\n","\n","**-> Mutually exclusive**\n","\n","Occurrence of one event means the other even cannot occur. In a deck of 52 cards, drawing a black card and drawing a hearts are mutually exclusive events because all the hearts are red. Getting a head and tail simultaneously when tossing a coin.\n","\n","If two events are mutually exclusive, they are not independent. Also, independent events cannot be mutually exclusive.\n","\n","**-> Equally likely events**\n","\n","2 or more events which have an equal probability of occurrence. eg: When a coin is tossed, there are equal chances of getting head or a tail.\n","\n","**-> Exhaustive events**\n","\n","A set of events are called exhaustive events if at least one of them necessarily occurs whenever the experiment is performed. Consider an example of an exhaustive occurrence. When rolling a dice, there are six possible outcomes: 1, 2, 3, 4, 5, 6. If we roll a dice, one of these six possibilities will almost certainly occur. As a result, all six outcomes are exhaustive occurrences. As a result, the union of the exhaustive events yields the complete sample space.\n","\n","**-> Complementary Events**\n","\n","For any event E1 there exists another event E1‘ which represents the remaining elements of the sample space S.\n","\n","E1 = S − E1‘\n","\n","If a dice is rolled then the sample space S is given as S = {1 , 2 , 3 , 4 , 5 , 6 }. If event E1 represents all the outcomes which is greater than 4, then E1 = {5, 6} and E1‘ = {1, 2, 3, 4}.\n","\n","Thus E1‘ is the complement of the event E1.\n","\n","\n","**-> Joint, Marginal, Conditional Probability**\n","\n","These are techniques used to quantify the probabilities for multiple random variables.\n","\n","1. Joint Probability is the P(2 events occuring simultaneously)\n","\n","Joint prob, P(A and B) = P(A|B) * P(B) = P(B|A) * P(A) = P(B and A)\n","\n","2. Marginal probability is the P(event irrespective of the outcome of another variable)\n","\n","Suppose there are 2 events X=A, Y=B. Then,\n","\n","marginal probability=P(A) for all outcomes of Y=Sum(P(X=A,Y=yi for all i))\n","\n","3. Conditional Probability is the P(event occurring after or in the presence of second event)\n","\n","Condition prob= P(A|B)\n","\n","eg: In an excel sheet, random variables are columns and events are rows.\n","\n","These probabilities form the basis of predictive modelling with problems such as classification, regression etc. For example,\n","\n","P(a row of data) is the joint prob across each input variable.\n","\n","P(a specific value of 1 input variable) is the marginal prob across the values of other input variable\n","\n","Predictive model itself is an estimate of conditional probability of an output given an input example.\n","\n","**-> Bayes theorem**\n","\n","The Bayes theorem is a mathematical formula for calculating conditional probability in probability and statistics. Used in Machine learning algorithms like Naive Bayes. One primary scientific value of Bayes's theorem today is in comparing models to data and selecting the best model given those data. For example, imagine two mathematical models, A and B, from which one can calculate the likelihood of any data given the model (p(D|A) and p(D|B)). For example, if a disease is related to age, then, using Bayes' theorem, a person's age can be used to more accurately assess the probability that they have the disease, compared to the assessment of the probability of disease made without knowledge of the person's age. Baye's theorem relies on consolidating prior probability distributions to generate posterior probabilities. Prior probability is the probability of an event before new data is collected."],"metadata":{"id":"CbSn5JI3Sf6Z"}},{"cell_type":"markdown","source":["# Formulas\n","\n","-> For any event E, P(E) ≥ 0\n","\n","-> Theoretical probability,\n","\n","P(E) = Number of Favourable Outcomes / Total Number of Outcomes\n","\n","-> Empirical/Experimental probability,\n","\n","P(E) = No: of times an event occurs / total number of trials\n","\n","-> P(S) = 1\n","\n","->  P(P ∪ Q) = P(P) + P(Q) – P(P ∩ Q)\n","\n","P(A ∪ B ∪ C) = P(A) + P(B) + P(C) − P(A · B) − P(A · C) − P(B · C) + P(A · B · C).\n","\n","-> If E and F are mutually exclusive events, then\n","\n","Joint prob, P(E and F) = 0\n","\n","P(E ∪ F) = P(E) + P(F)\n","\n","-> P(not M) = 1 – P(M)\n","\n","-> P(sure event) = 1\n","\n","-> P(impossible event) = 0\n","\n","-> P(E|F) = P(E ∩ F)/P(F), provided P(F) ≠ 0\n","\n","-> For 2 independent events E and F,\n","\n","Joint probability, P (E ∩ F) = P (E).P(F)\n","\n","Marginal probability = P(E)\n","\n","Conditional probability = P(E)\n","\n","To prove 2 events are independent : P(E|F)=P(E)\n","\n","-> Bayes Theorem\n","\n","P(A|B) = (P(B|A) * P(A))/ P(B)\n","\n","P(A ∣ B) is the conditional probability of event A occurring, given that B is true.\n","\n","P(B ∣ A) is the conditional probability of event B occurring, given that A is true.\n","\n","P(A) and P(B) are the probabilities of A and B occurring independently of one another.\n","\n","-> Extended Bayes theorem\n","\n","P(A|B,C) = (P(B|A and C) * P(A|C))/P(B|C)\n","\n","-> Mean absolute deviation\n","\n","1/n * (sigma i=1 to n |xi-x_bar|)\n","\n","-> Median absolute deviation\n","\n","median(|xi - x_median|)\n"],"metadata":{"id":"DqOLVpZ0Zt3o"}},{"cell_type":"markdown","source":["# Probability Distribution\n","\n","Probability distribution is the mathematical function that gives the probabilities of occurrence of different possible outcomes of an experiment. Its used to compare the relative occurrence of many different random values. Random variable takes values from sample space. Prob dist belongs to 2 classes:-\n","\n","1. **Discrete Probability Distribution**- Applicable to scenarios where the set of possible outcomes is discrete eg: coin toss, rolling dice . Probabilities are described by Probability Mass Function, PMF. It assigns a probability to each possible outcome. eg: Poisson, Bernoulli, Binomial, Geometric\n","\n","2. **Continous Probability Distribution** - Possible outcomes can take on values in a contionous range. Probabilities are described by Probability Density Function, PDF. Probability distribution is by definition the integral of the PDF. Probability that the outcome lies in an interval can be computed by integrating the PDF over that interval. eg: Normal, Uniform, Chi Squared\n","\n","-> **Cumulative Distribution Function (CDF)** describes the probability that the random variable is no larger than a given value. ie P(X < x) for some x. CDF is the area under the PDF from -inf to x.\n","\n","-> **Tail** - Regions close to bounds of random variable\n","\n","-> **Expected Value or mean** - Weighted average of possible values using their probabilities as weights\n","\n","-> **Median** - The value such that probability of both, set of values less than the median and set of values greater than median are 1/2\n","\n","-> **Mode** - Value with highest probability\n","\n","-> **Quantile** - q-quantile is the value x such that P(X < x)=q\n","\n","-> **Variance, Standard deviation** - Measures of dispersion. Variance and standard deviation allow you to quickly understand how close most of a population is to the mean.\n","\n","-> **Symmetry** - Property of some distributions in which portion of distribution to the left of a specific value is a mirror image of portion of distribution to its right.\n","\n","-> **Skewness** - Measure of the extent to which a pmf/pdf leans to one side of its mean. It measures the degree of symmetry of a distribution.\n","\n","Symmetric distributions has a skewness of 0. eg: Normal distribution\n","\n","Skewness < 0 or -ve skewness means left tail is longer\n","\n","Skewness > 0 or +ve skewness means right tail is longer\n","\n","-> **Kurtosis** - Measures fatness of tails of pmf/pdf. Kurtosis indicates how much data resides in the tails. Distributions with a large kurtosis have more tail data than normally distributed data, which appears to bring the tails in toward the mean. Distributions with low kurtosis have fewer tail data, which appears to push the tails of the bell curve away from the mean.\n","\n","Kurtosis is a measure of the combined weight of a distribution's tails relative to the center of the distribution curve (the mean). For example, when a set of approximately normal data is graphed via a histogram, it shows a bell peak, with most of the data residing within three standard deviations (plus or minus) of the mean. However, when high kurtosis is present, the tails extend farther than the three standard deviations of the normal bell-curved distribution.\n","\n","kurtosis = 3 mesokurtic (normal)\n","\n","kurtosis > 3 leptokurtic (more than normal)\n","\n","kurtosis < 3 platykurtic (less than normal)\n","\n","A curve's kurtosis characteristic tells you how much kurtosis risk the investment you're evaluating has.\n","\n","**Different probability distributions**\n","\n","**-> Linear growth (eg: errors)**\n","\n","1. Normal/Gaussian distribution\n","\n","* X~N(mu,sigma^2)\n","\n","* Its a probability distribution that is symmetric about the mean showing that data nearthe mean are more frequent in occurrence than data far from the mean. Continous probability distribution. For single such quantity. Its a bell shaped curve. Probabilities for normal distribution can be computed using statistical tables for the standard normal probability distribution\n","\n","* Mean=Median=Mode is the highest point in distribution graph. Width of graph is defined by the std deviation.\n","\n","* Standard normal distribution - Normal probability distribution with a mean of 0, std deviation 1, skewness 0 and kurtosis 3\n","\n","* PDF, f(x)=1/(sigma*sqrt(2*pi)) * e^-((x-mu)^2/(2*sigma^2))\n","\n","* Normal distributions are symmetrical but not all symmetrical distributions are normal.\n","\n","* Empirical Rule\n","\n","For all normal distributions, 68.2% of the observations will appear within +/- 1 std deviation of the mean, 95.4% within +/- 2 std deviation of mean, 99.7% appear within +/- 3 std deviations.\n","\n","Z score=(x-mu)/sigma tells how many std deviations below the mean the data point is. Used to standardize the distribution. Negative z score says point lies below the mean and positive value says point lies above the mean.\n","\n","Z score table helps to know the % of values below a z score in standard normal distribution. Table entry for z is the area under the std normal curve  to the left of z. About 68% of the elements have a z score -1 and +1, 95% have b/w -2 and +2, 99% have b/w -3 and +3\n","\n","Many naturally occurring phenomena appear to be normally distributed. eg: distribution of heights of human beings. Taller and shorter people exists but with decreasing frequency.\n","\n","**-> Exponential growth (eg: prices, incomes, populations)**\n","\n","1. Log-normal distribution\n","\n","For single such quantity whose log is normally distributed. Log normal distribution is a right skewed continous prob distribution. Used for modelling income distributions, length of chess games, stock market returns. Prices tend to follow more of a log normal distribution that is right skewed and with fatter tails.\n","\n","Example:\n","\n","Length of Youtube comments\n","\n","Time spent on Youtube\n","\n","Delivery time of Zomato\n","\n","When we take log of log normal data we end up with a normal distribution. If Y has a normal distribution, we take the exponential of Y (X=exp(Y)), then we get back to X variable which has log normal distribution.\n","\n","PDF, f(x) = 1/(x*sigma*sq.rt(2*pi)) * e^-1/2 * ((ln(x)-mu)/sigma)^2\n","\n","2. Pareto distribution\n","\n","For single such quantity whose log is exponentially distributed. The power law distribution.\n","\n","**-> Uniformly distributed quantities**\n","\n","1. Discrete uniform distribution\n","\n","For finite set of values eg: outcome of fair dice.\n","\n","2. Continous uniform distribution\n","\n","For absolutely continously distributed values.\n","\n","**-> For Bernoulli trials - yes/no events with a given probability**\n","\n","1. Bernoulli distribution\n","\n","Discrete prob distbn. Its for the outcome of a single Bernulli trial eg: success/failure, yes/no . X ~ Bernoulli(p)\n","\n","eg: Repeated tossing of coin\n","\n","Drawing balls with replacement is a bernoulli trial and without replacement is not a bernoulli trial.\n","\n","2. Binomial distribution\n","\n","-> Discrete probability distribution. For the number of positive occurrences (eg: successes,yes,votes) given a fixed total number of independent occurrences\n","X~Binomial(n,p)\n","\n","-> Properties\n","\n","* It consists of a sequence of n identical trials\n","\n","* 2 outcomes, success or failure are possible on each trial\n","\n","* P(success)=p doesnt change from trial to trial\n","\n","* Trials are independent.\n","\n","The probability mass function, f(x)=P[X=x]=(nCx)p^x(1−p)^n−x.\n","\n","3. Geometric distribution\n","\n","Discrete probability distribution. For the number of failures before the first success. The distribution is a set of probabilities that presents the chances of success after 0 failures, 1 failure, 2 failures etc.\n","\n","* Series of trials\n","\n","* Each trial has only 2 possible outcomes - success/failure\n","\n","* P(success) is same for each trial\n","\n","Probability that first success occurs at k+1 after k number of trials,\n","\n","PMF, P(X=k) = (1-p)^k * p\n","\n","CDF, P(X<=k) = sigma i=0 to k (1-p)^i * P\n","\n","Example:\n","\n","Number of customers to be approached before a successful sale.\n","\n","**-> Poisson process (events that occur independently with a given rate)**\n","\n","1. Poisson distribution\n","\n","X~Poisson(lambda)\n","\n","Discrete probability distribution. For the number of occurrences of a poisson type event in a given period of time. eg: Random variable defined as number of telephone calls coming to an airline reservation system during a period of 15 minutes.\n","\n","The formula for Poisson distribution (prob of observing x events over period) is f(x) = P(X=x) = (e^-λ * λ^x )/x!. For the Poisson distribution, λ is always greater than 0. For Poisson distribution, the mean and the variance of the distribution are equal.\n","\n","* The experiment counts the number of occurrences of an event over an interval\n","\n","* Occurrence of 1 event doesn't affect the proba that a second event will occur\n","\n","* The average rate at which events occur is independent of any occurrences (lambda)\n","\n","* No 2 events occur simultaneously.\n","\n","Mean=Variance=lambda\n","\n","Scenario:\n","\n","To determine the amount of staffing that's needed in a call center. A call center receives an average of 4.5 calls every 5 minutes. Each agent can handle one of these calls over the 5 mt period. If no agent is free, call will be placed on hold. We need to find out the minimum number of agents needed on duty so that calls are placed on hold at most 10% of time. If X is the number of calls received and k is the number of agents, then k should be set such that    P(X>k)=0.1 or P(X<=k)=0.9 . We can calculate P(X=0),P(X=1),P(X=2) etc.. Similarly caluculate P(X<=1)=P(X=0)+P(X=1),P(X<=2)..P(X<=k) such that this value becomes greater than 0.9\n","\n","Examples:\n","\n","* No: of arrivals at a car wash in 1 hour\n","\n","* No: of n/w failures per day\n","\n","* No: of births, deaths, marriages, divorces over a given period of time.\n","\n","* No of visitors to a website per minute\n","\n","2. Exponential distribution\n","\n","X~Exponential(lambda)\n","\n","Continous probability distribution. For the time before the next poisson type event occurs. To model the time elapsed between events.\n","\n","x represents time here.\n","\n","PDF f(x) = P(X=x) = lambda*e^(-lambda*x) if x>=0 otherwise 0\n","\n","CDF F(x) = P(X<=x) = 1-e^(-lambda*x) if x>=0 otherwise 0\n","\n","Mean=1/lambda\n","\n","Variance=1/(lambda)^2\n","\n","**-> Normally distributed quantities operated with sum of sqaures**\n","\n","1. Chi-squared distribution\n","\n","Contionous prob distbn. Distribution of a sum of squared standard normal variables. Useful for inference regarding the sample variance of normally distributed samples.\n","\n","2. Student's t distribution\n","\n","Continous prob distbn. Distribution of the ratio of a standard normal variable and the square root of a scaled chi squared variable. Useful for inference regarding the mean of normally distributed samples with unknown variance.\n","\n","3. F distribution\n","\n","Continous prob distbn. Distribution of the ratio of 2 scaled chi squared variables. Useful for inferences that involvecomparing variances.\n","\n","**-> Central Limit Theorem**\n","\n","-> CLT states that for any population with mean mu and std deviation sigma, the distribution of the sample mean for sample size N has mean mu and std deviation sigma/sq.rt(N). When the sample size is large, the distribution of sample mean will be normal regardless of original distribution of population\n","\n","-> Assumptions:\n","\n","* Sample is drawn randomly\n","\n","* Sample drawn should be indep of each other. They shouldn't influence other samples.\n","\n","* When sampling is done without replacement, sample size shouldnt exceed 10% of the total population.\n","\n","* Sample size should be sufficiently large.\n","\n","* Population distribution has finite variance - If you sample from a population with finite variance, then s^2 will converge to the population variance. That is, once sample size = n gets reasonably big (say 40 or 50), the value of s^2 is not going to move around very much. A sufficiently large sample size can predict the characteristics of a population more accurately.\n","\n","As a general rule, sample sizes of 30 are typically deemed sufficient for the CLT to hold, meaning that the distribution of the sample means is fairly normally distributed. With a sufficiently large sample size, the sample distribution will approximate a normal distribution, and the sample mean will approach the population mean. So if we have a sample size of at least 30, we can begin to analyze the data as if it fit a normal distribution. It implies that probabilistic and statistical methods that work for normal distributions can be applicable to many problems involving other types of distributions.\n","\n","-> CLT for sample means, Z= (x_bar-mu)/(sigma/sq.rt(N)) .\n","\n","X_bar ~ N(mu,sigma/sq.rt(N)). This is used to find z score from Z table if N>30 otherwise t score is calculated from t table.\n","\n","-> Used in hypothesis testing, creating confidence intervals. Concept of CLT is used in election polls to estimate the percentage of people supporting a particular candidate as confidence intervals. Used in calculating mean family income in a particular country.\n","\n","CI=x_bar+/-(Z*S)/sq.rt(n) where Z=(x-mu)/sigma\n","\n","X_bar-sample mean\n","\n","S-sample std deviation\n","\n","Z-Z score, number of std deviations from sample mean\n","\n","n-size of sample\n","\n","Value after +/- is margin of error\n","\n","-> Z scores for 90%-1.645, 95%-1.96, 99%-2.58\n","\n","**-> Confidence Interval**\n","\n","Confidence intervals measure the degree of uncertainty or certainty in a sampling method. Confidence intervals are conducted using statistical methods, such as a t-test.\n","\n","Confidence interval is the range of values that we expect our estimate to fall between a certain % of time if we run our experiment again or resample the population in the same way.\n","\n","Confidence level is the % of times we expect to reproduce an estimate b/w the upper and lower bounds of the confidence interval and is set by the alpha value.\n","\n","Confidence level=1-alpha eg: if alpha=0.05, confidence level=1-0.05=0.95=95%\n","\n","Confidence intervals are used to measure the uncertainity in a sample variable. The confidence is in the method.\n","\n","Suppose a group of researchers is studying the heights of high school basketball players. The researchers take a random sample from the population and establish a mean height of 74 inches.\n","\n","The mean of 74 inches is a point estimate of the population mean. A point estimate by itself is of limited usefulness because it does not reveal the uncertainty associated with the estimate; you do not have a good sense of how far away this 74-inch sample mean might be from the population mean. What's missing is the degree of uncertainty in this single sample.\n","\n","Confidence intervals provide more information than point estimates. By establishing a 95% confidence interval using the sample's mean and standard deviation, and assuming a normal distribution as represented by the bell curve, the researchers arrive at an upper and lower bound that contains the true mean 95% of the time.\n","\n","Assume the interval is between 72 inches and 76 inches. If the researchers take 100 random samples from the population of high school basketball players as a whole, the mean should fall between 72 and 76 inches in 95 of those samples.\n","\n","If the researchers want even greater confidence, they can expand the interval to 99% confidence. Doing so invariably creates a broader range, as it makes room for a greater number of sample means. If they establish the 99% confidence interval as being between 70 inches and 78 inches, they can expect 99 of 100 samples evaluated to contain a mean value between these numbers.\n","\n"," A 90% confidence level, on the other hand, implies that we would expect 90% of the interval estimates to include the population parameter, and so forth.\n","\n"," A confidence interval is a range of values, bounded above and below the statistic's mean, that likely would contain an unknown population parameter. Confidence level refers to the percentage of probability, or certainty, that the confidence interval would contain the true population parameter when you draw a random sample many times.\n","\n"," Statisticians use confidence intervals to measure uncertainty in a sample variable. For example, a researcher selects different samples randomly from the same population and computes a confidence interval for each sample to see how it may represent the true value of the population variable. The resulting datasets are all different where some intervals include the true population parameter and others do not.\n","\n"," Confidence intervals are conducted using statistical methods, such as a t-test. A t-test is a type of inferential statistic used to determine if there is a significant difference between the means of two groups, which may be related to certain features. Calculating a t-test requires three key data values. They include the difference between the mean values from each data set (called the mean difference), the standard deviation of each group, and the number of data values of each group."],"metadata":{"id":"2NkPYswqLzed"}},{"cell_type":"markdown","source":["## Transformations for Normal distribution\n","\n","Transformation means taking a mathematical function and applying it to the data. Numerical variables may have high skewed and non normal distribution caused by outliers, highly exponential distributions etc. There we go for data transformation.\n","\n","1. Log transformation\n","\n","X replaced by log(x) with base 10, base2, or natural log\n","\n","log_traget=np.log1p(df['target'])\n","\n","2. Square root transformation\n","\n","sqrt_log=df['target']**0.5\n","\n","It can be applied to zero values too.\n","\n","3. Reciprocal transformation\n","\n","reci_target=1/df['target']\n","\n","Can be used only for non zero values\n","\n","4. Box-Cox transformation\n","\n","y(lambda) = ((y^lambda)-1)/lambda if lambda!=0 else log y if lambda=0\n","\n","Values from -5 to 5 are considered for lambda and best value that gives best skewness is chosen.\n","\n","from scipy.stats import boxcox\n","\n","bcx_target, lam =boxcox(df['target'])\n","\n","lam will give the best lambda value. For this, values should be strictly positive\n","\n","5. Yeo-Johnson transformation\n","\n","This doesn't require the values to be strictly +ve.\n","\n","from scipy.stats import yeojohnson\n","\n","yf_target, lam = yeojohnson(df['target'])\n","\n","## Normality test\n","\n","A normality test determines whether a sample data has been drawn from a normally distributed population. Parametric tests are used when the distribution closely follows the normal distribution; otherwise, non-parametric tests are employed. For two groups of data, the most widely used parametric test is the t-test (for independent or paired samples, depending on our data), and the non-parametric equivalent is the Mann–Whitney test. For more than two groups of data, the parametric test used is ANOVA, and its non-parametric equivalent is the Kruskal–Wallis test. For correlations, the parametric test used is the Pearson correlation test, and its non‑parametric equivalent is the Spearman rank-correlation test. Ways to test normality-\n","\n","1. Histogram\n","\n","2. Q-Q probability plot\n","\n","3. Shapiro–Wilk test\n","\n","4. Kolmogorov–Smirnov test\n","\n","**-> Quantile Quantile plots**\n","\n","QQ plot is a graphical method for determining\n","\n","1. whether 2 samples of data came from the same population or not.\n","\n","2. Whether 2 samples have the same distribution shape.\n","\n","A qq plot is a plot of the quantiles of the first data set against the quantiles of the second data set (or theoretical quantiles, z scores against actual quantiles from sample data). A 45 degree line is also plotted. If the samples are from the same population then the points are along this line.\n","\n","**-> KS(Kolmogorov Smirnov Test)**\n","\n","Its used as a test of goodness of fit.\n","\n","* KS One sample test\n","\n","It compares the cumulative distribution function for a variable with a specified distribution. H0 is no difference between the observed and theoretical distribution.\n","\n","* KS 2 sample test\n","\n","Test is used to test the agreement between 2 cumulative distributions. H0 states there's no difference between the 2 distributions.\n","\n","**-> Shapiro test**\n","\n","The Shapiro-Wilk test is a hypothesis test that is applied to a sample with a null hypothesis that the sample has been generated from a normal distribution. If the p-value is low, we can reject such a null hypothesis and say that the sample has not been generated from a normal distribution.\n","\n","The Shapiro Wilk test is more appropriate method for small sample sizes (< 50 samples) although it can also be handling on larger sample size while Kolmogorov Smirnov test is used for n ≥50. For both of the above tests, null hypothesis states that data are taken from normal distributed population."],"metadata":{"id":"eRaejJIuwSOf"}},{"cell_type":"markdown","source":["# Formulas\n","\n","1. PMF,f(x) = P(X=x)\n","\n","2. PDF,p(x) = d/dx (F(x)) = F'(x) where F(x) is the CDF\n","\n","3. CDF,F(x) = p(X<=x) = integral -inf to x p(x).dx\n","\n","This is the probability that the R.V is no larger than a given value.\n","\n","4. Quantile Function / PPF - Probability Point Function\n","\n","F(x) = P(X<=x) = p\n","\n","Q(p) = F_inv(p)\n","\n","It maps its input p to a threshold value x such that P(X<=x)=p\n","\n","5. Expectation\n","\n","For continous random variable = integral x.f(x).dx where f(x) is pdf\n","\n","For discrete random variable = sigma x.p(x) where p(x) is pmf\n","\n","6. Variance (sigma^2)\n","\n","For discrete R.V, sigma(x-mu)^2.f(x) where f(x) is the pmf\n","\n","For continous R.V, integral(x-mu)^2.p(x).d(x) where p(x) is the pdf\n","\n","E(x^2) - (E(x))^2"],"metadata":{"id":"4SEx7yGbdCXD"}},{"cell_type":"markdown","source":["# Estimation\n","\n","-> Subset of population is called sample.\n","\n","-> Characteristics like population mean, variance, proportion are called parameters of population. Characteristics like sample mean, variance and proportion are sample statistics.\n","\n","-> 2 types of estimates - Point and interval\n","\n","1. Point\n","\n","Point estimate is a value of sample statistic that is used as a single estimate of a population parameter.\n","\n","2. Interval\n","\n","Interval estimates are accompanied by a statement concerning the degree of confidence that the interval contains the population parameter being estimated. Interval estimates of popultaion parameters are called confidence intervals.\n","\n","-> Sampling distribution is the probability distribution for a sample statistic. Sampling distribution is needed for the construction of an interval estimate for a population parameter.\n","\n","-> Fundamental point and interval estimation is estimation of population mean\n","\n","x_bar - sample mean\n","\n","mu - population mean\n","\n","|x_bar-mu| - sampling error\n","\n","sigma - population std deviation\n","\n","Std deviation of sampling distribution is called standard error\n","\n","Mean of sampling distribution of x_bar is equal to population mean,mu and the std deviation is sigma/sq. root of n\n","\n","sigma/sq. root of n - standard error\n","\n","For 95% CI, 1.96*standard error - margin error\n","\n","95% CI estimate for population mean = x_bar +- 1.96*sigma/sq.root(n)\n","90% CI estimate for population mean = x_bar +- 1.645*sigma/sq.root(n)\n","\n","90% CI is narrower than a 95% CI.\n","\n","For n < 30, t distribution is used.\n","\n","-> For categorical variables, population proportion is a parameter of interest.\n","\n","-> Estimation for 2 populations\n","\n","Sampling distribution of x1_bar-x2_bar (difference between 2 sample means) would provide the basis for a CI estimate of difference between 2 population means. eg: to determine difference between salaries paid to a population of men and women.\n","\n","For categorical vars, difference between population proportions."],"metadata":{"id":"BbvXfM9zYUVe"}},{"cell_type":"markdown","source":["# Hypothesis testing\n","\n","-> Form of statistical inference that uses data from a sample to draw conclusions about a population parameter or a population probability distribution. A hypothesis test can be performed on parameters of one or more populations, also in regression and correlation analysis to determine if the regression relationship and the correlation coefficient are statistically significant, also in goodness-of-fit which is a hypothesis test in which H0 is that population has a specific probability distribution such as normal P.D.\n","\n","H0 is null hypothesis, opposite of that is alternate hypothesis Ha. Hypothesis testing involves using sample data to determine whether or not H0 can be rejected.\n","\n","Type 1 error - Rejecting H0 when its actually true. P(making type 1 error)=alpha\n","\n","Type 2 error - Accepting H0 when its actually false. P(making type 2 error)=beta\n","\n","-> **Steps**\n","\n","1. Define Null and Alternative Hypothesis\n","\n","State the null hypothesis (H0), representing no effect, and the alternative hypothesis (H1​), suggesting an effect or difference.\n","\n","There are three types of hypothesis tests: right-tailed, left-tailed, and two-tailed. Right-tailed tests assess if a parameter is greater, left-tailed if lesser. Two-tailed tests check for non-directional differences, greater or lesser. One tailed allows for the possibility of rejection of H0 in only one direction. Entire level of significance has either in the left tail or right tail. 2-tailed tests the possibility of rejection in both directions.It splits the level of significance into half.\n","\n","2. Choose significance level\n","\n","Select a significance level (\\alpha   ), typically 0.05, to determine the threshold for rejecting the null hypothesis.\n","\n","3. Collect and Analyze data.\n","\n","4. Calculate Test Statistic\n","\n","There are various hypothesis tests, each appropriate for various goal to calculate our test.\n","\n","* Z-test: If population variance is known or population variance is unknown but sample size >= 30. Its a test for population mean or proportion.\n","\n","**A)** z=(x_bar-mu)/(sigma/sq.rt(n))\n","\n","x_bar-sample mean\n","\n","mu-population mean\n","\n","sigma-population std deviation\n","\n","n-sample size\n","\n","**B)** For 2 sample z test,\n","\n","z=((x1_bar-x2_bar)-(mu1-mu2))/sq.rt(sigma1^2/n1 + sigma2^2/n2)\n","\n","**C)** One proportion Z test,\n","\n","z=(p_hat-p0)/sq.rt(p0*(1-p0)/n)\n","\n","p0-null hypothesized value\n","\n","p_hat-observed proportion\n","\n","* t-test: If population variance is unknown and sample size < 30. Test for population mean.\n","\n","**A)** t=(x_bar-mu)/(s/sq.rt(n)) with dof=n-1\n","\n","**B)** For 2 sample t test,\n","\n","t=((x1_bar-x2_bar)-(mu1-mu2))/sq.rt(s1^2/n1 + s2^2/n2) with dof=n1+n2-2\n","\n","s-sample standard deviation\n","\n","* Chi-square test: Chi-square test is used for categorical data or for testing independence in contingency tables. Used mainly when sample size is large enough.\n","\n","chi^2=sum(Oij-Eij)^2/Eij\n","\n","Oij-Observed frequency in cell\n","Eij-Expected frequency in cell=(row total*col total)/(total no of observations)\n","\n","* F-test: F-test (Fisher analysis of variance) is often used in analysis of variance (ANOVA) to compare variances or test the equality of means across multiple groups. In a regression study, ANOVA is used to determine the influence that independent variables have on the dependent variable.\n","\n","ANOVA coefficient, F=MST/MSE\n","\n","MST-Mean sum of squares due to treatment\n","\n","MSE-Mean sum of squares due to error\n","\n","ANOVA test allows a comparison of 2 or more groups at the same time to determine whether a relationship exists between them. It is similar to multiple 2 sample t tests. It results in fewer type 1 errors. ANOVA assumes data is normally distributed.2 main types of ANOVA are one way and two way. It refers to number of independent variables. A different variation is MANOVA - Multivariate ANOVA. It tests for multiple dependent variables.\n","\n","5. Comparing Test Statistic\n","\n","If Test Statistic>Critical Value: Reject the null hypothesis.\n","\n","If Test Statistic≤Critical Value: Fail to reject the null hypothesis.\n","\n","To determine critical values for hypothesis testing, we typically refer to a statistical distribution table , such as the normal distribution or t-distribution tables based on.\n","\n","We can also come to an conclusion using the p-value,\n","\n","If the p-value < alpha, you reject the null hypothesis. This indicates that the observed results are unlikely to have occurred by chance alone, providing evidence in favor of the alternative hypothesis.\n","\n","If the p-value>=alpha, you fail to reject the null hypothesis. This suggests that the observed results are consistent with what would be expected under the null hypothesis.\n","\n","The p-value is the probability of obtaining a test statistic as extreme as, or more extreme than, the one observed in the sample, assuming the null hypothesis is true. To determine p-value for hypothesis testing, we typically refer to a statistical distribution table , such as the normal distribution or t-distribution tables based on.\n","\n","Critical value(Tcr) is the point in pdf for which area under the curve to the right is equal to alpha. Its the cut off value between acceptance zone and rejection zone. We compare our test score Tobs to Tcr and if Tobs>Tcr, that means Tobs lies in the rejection zone and we reject H0 otherwise we fail to reject H0.\n","\n","6. Interpret the results\n","\n","-> **Example scenario**\n","\n","Case A: Does a New Drug Affect Blood Pressure?\n","\n","Imagine a pharmaceutical company has developed a new drug that they believe can effectively lower blood pressure in patients with hypertension. Before bringing the drug to market, they need to conduct a study to assess its impact on blood pressure.\n","\n","Data will be pressure readings of patients before and after treatment.\n","\n","Null Hypothesis: (H0)The new drug has no effect on blood pressure.\n","Alternate Hypothesis: (H1)The new drug has an effect on blood pressure.\n","\n","Significance level decided at 0.05\n","\n","To compute the test statistic we use paired T-test. The test statistic is calculated based on the differences between blood pressure measurements before and after treatment.\n","\n","t = m/(s/√n)\n","\n","m = mean of the difference\n","s = standard deviation of the difference\n","n = sample size,\n","\n","Using the calculated t-statistic and degrees of freedom df = n-1, you can find the p-value using statistical software or a t-distribution table.\n","\n","Based on p value come to conclusion.\n","\n","-> **Level of significance** , alpha of hypothesis test is the maximum allowable probability of making a type 1 error. Common choices are 0.05 and 0.01\n","Lowering alpha will minimize type 1 error and increasing alpha will minimize type 2 error.\n","\n","-> A graph known as **operating characteristic curve** can be constructed to show how changes in the sample size affect the probability of making type-2 error.\n","\n","-> **P-value**\n","\n","Measure of how likely the sample results are assuming H0 is true. Smaller p value means less likely the sample results. p values is often called observed level of significance.\n","\n","p_value < alpha => H0 can be rejected\n","p_value >= alpha => H0 cannot be rejected.\n","\n","p-value=P(data at least as extreme as actual observation|H0)\n","\n","-> **Notes**\n","\n","Statistical method to evaluate the performance and validity of machine learning models. Tests specific hypotheses about model behavior, like whether features influence predictions or if a model generalizes well to unseen data."],"metadata":{"id":"ANgneC1Zdkpp"}},{"cell_type":"markdown","source":["## A/B Testing\n","\n","Its used to test a subject's response to variant Aagainst variant B and determining which of the variants is more effective. It is used to optimize web marketing strategies. To choose the best design for website by looking at the analytics results obtained with 2 possible alternatives A and B. Its useful for understanding user engagement and satisfaction of online features like a new feature or product.\n","\n","-> Discrete/Binomial metrics example\n","\n","Click through rate, Conversion rate, Bounce rate\n","\n","-> Continous/Non binomial metrics example\n","\n","Average revenue per user, average session duration, average order value\n","\n","To compare efficacy of 2 designs, 2 sample hypothesis test is performed. For discrete metrics, if sample size is large chi square test otherwise F test. For continous metric,large sample size and normal distributions, z test is used if variance is known otherwise t test is used.\n","\n","## Covariance\n","\n","-> Covariance is a statistical term that refers to a systematic relationship between 2 random variables in which a change in the other reflects a change in one variable.\n","\n","-> Ranges from -inf to +inf. Negative value indicating negative relationship and positive value indicating positive relationship. Positive covariance means they are heading in same direction. Negative covariance means variables shift in opposite direction.\n","\n","covariance(x,y) = 1/n * sum((xi-x_bar)*(yi-y_bar))\n","\n","xi,yi-data value of x,y\n","\n","x_bar,y_bar-mean of x,y\n","\n","-> Application\n","\n","TO reduce the dimensions of large data sets, PCA is used. To perform PCA, an eigen decomposition is applied to the covariance matrix.\n","\n","## Correlation Matrix\n","\n","-> A correlation matrix can be defined as a matrix with correlation coefficients among different variables. Correlation is a measure that determines the degree to which 2 or more random variables move in sequence. It describes how a change in 1 variable leads to a change in the percentage of the second variable. +ve correlation occurs when 2 variables move in the same direction. -ve correlation when they move in opposite direction.\n","\n","corerlation(x,y)=cov(x,y)/(sq.rt(var(x))*sq.rt(var(y)))\n","\n","-> Ranges from -1 to +1\n","\n","-> Correlation matrix is used to look for a pattern in the data and determine whether the variables are highly correlated.\n","\n","-> Both correlation and covariance measure only the linear relationships between 2 variables. Correlation is preferred over covariance because it doesnt get affected by the change in scale.\n","\n","##Pearson correlation coefficient (r)\n","\n","It a descriptive statistic that describes the strength and direction of the linear relationship between 2 quantitative variables. Its also an inferential statistic meaning that it can be used to test statistical hypothesis. We can test whether there's a significant relationship between 2 variables. Its also a measure of how close the observations are to a line of best fit. It also tells whether the slope of the line of best fit is negative or positive. When r=1 or -1, all points fall exactly on the line of best fit. When r=0 line of best fit is not helpful in describing the relationship between variables ie ther's no linear relationship. PCC is used when,\n","\n","1. Both variables are quantitative\n","\n","2. Variables are normally distributed (Create a histogram and check)\n","\n","3. Data have no outliers. (Check using scatter plot)\n","\n","4. Relationship is linear. (Check using scatterplot)\n","\n","r=n*sum(xy)-(sum(x)*sum(y))/sq.rt((n*sum(x^2)-sum(x)^2)*(n*sum(y^2)-sum(y)^2))\n","\n","positive correlation - baby length and weight\n","\n","negative correlation - elevation and air pressure\n","\n","no correlation - car price and width of windshield wipers\n","\n","## Spearman's rank correlation coefficient\n","\n","rs=1-(6*sum(di^2)/n*(n^2-1))\n","\n","di-difference in the ranks given to the 2 variable values for each item of the data.\n","\n","-> Used when\n","\n","1. Variables are ordinal\n","\n","2. Variables aren't normally distributed\n","\n","3. Data includes outliers\n","\n","4. Relationship between variables is non linear and monotonic.\n","\n","-> Its robust to outliers present in the data. Its superior for calculating qualitative observations.\n","\n","## Feature Scaling\n","\n","Algorithms that compute the distance between the features are biased towards numerically larger values if the data is not scaled. ALso feature scaling helps ml,dl algorithms train and converge faster. Normalization and standardisation are most common feature scaling techniques.\n","\n","### Normalization or Min-Max scaling\n","\n","X_new=(X-X_min)/(X_max-X_min)\n","\n","Scales the range to [0,1]. Useful when there are no outliers or when we dont know about the distribution. Used when features are of different scales.\n","\n","### Standardisation\n","\n","X_new=(X-mean)/s.d\n","\n","Useful in cases where data follows Gaussian distribution. Less affected by outliers. Used when we want to ensure 0 mean and unit std deviation. Not bounded to any range.\n","\n","### Robust scaling\n","\n","Robust to outliers\n","\n","x_new=(X-median)/IQR\n","\n"],"metadata":{"id":"FrFs7-mPQDtN"}},{"cell_type":"markdown","source":["# Visualizations\n","\n","## Bar Diagram\n","\n","Bar Diagrams represent information using a sequence of bars spanning two axes. The x-axis (the horizontal) categorizes the data into a group, with one bar representing each group. On the Y-axis, the exact numerical value of the given group is described. The bar chart is a visual representation of the frequency table, and the heights of the bars show how many times each value appears in the dataset. Bars will be equally spaced.\n","\n","## Pie Chart\n","\n","Essentially, a pie chart shows data as circular bars, with each slice representing a portion of the data. A pie chart is a visual representation of data in the form of numerical and categorical variables. According to a pie chart, the angle, area, and length of each slice are proportional to the amount it represents. So, it’s all about the definition of Pie Chart and bar diagram. Each slice represents a percentage or proportion of the whole. However, remember not to use these types of charts for large datasets, as too many slices can create confusion. The chart is suitable when you have limited categories, ideally less than six or seven.\n","\n","## Histogram\n","\n","A histogram is a visual representation of the distribution of quantitative data. A histogram is a graph used to represent the frequency distribution of a few data points of one variable. Histograms often classify data into various “bins” or “range groups” and count how many data points belong to each of those bins. Values of quantitative variable are shown on horizontal axis. Base of rectangle is equal to width of class interval and height is proportional to number of data values. Histograms give a rough sense of the density of the underlying distribution of the data, and often for density estimation: estimating the probability density function of the underlying variable. The total area of a histogram used for probability density is always normalized to 1.\n","\n","##Line Charts\n","\n","A line chart connects distinct data points through straight lines. Its best use case is to illuminate trends, patterns, and variable changes.This type of chart is also effective for demonstrating progression, making them suitable for scenarios like project timelines, production cycles, or population growth.\n","\n","## Scatter plots\n","\n","Scatter plots are types of visualization that show a collection of data points scattered around the graph. The data points can be evenly or unevenly distributed. Scatter plots are ideal for exploring relationships and patterns between two continuous variables. They can help you identify trends, correlations, or potential clusters in the data.\n","\n","## Violin plot and Box plot\n","\n"," A violin plot is a hybrid of a box plot and a kernel density plot, which shows peaks in the data. It is used to visualize the distribution of numerical data. Unlike a box plot that can only show summary statistics, violin plots depict summary statistics and the density of each variable. Violin plots can be used for univariate and bivariate analysis.In bivariate analysis, violin plots are utilized to examine the relationship between a continuous variable and a categorical variable. The categorical variable is represented on the x-axis, while the y-axis represents the values of the continuous variable. By creating separate violins for each category, the plot visualizes the distribution of the continuous variable for different categories."],"metadata":{"id":"MDDB8FA4btrn"}},{"cell_type":"code","source":[],"metadata":{"id":"hLA4sRwxjx1s"},"execution_count":null,"outputs":[]}]}