{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPgsaF+hEmzcEpus8mNtodX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Recommender Systems\n","\n","## -> Why recommender systems ?\n","\n","1. Increase in revenue based on personalization\n","\n","2. Better user experience\n","\n","3. More time spent on the platform\n","\n","4. Help websites improve user engagement.\n","\n","## -> Types of recommender systems\n","\n","**1. Market-Basket Analysis using Apriori Algorithm**\n","\n","-> Market basket analysis is used to analyze the combination of products which have been bought together. Used for purchases done by a customer.\n","\n","-> Apriori algorithm is used to calculate the association rules between objects that means how two or more objects are related to each other. Its a frequency based algorithm. eg: People who bought iPhones also bought Airpods.\n","\n","Used in telecommunications, banking/insurance, medical, ecommerce, retail\n","\n","-> IF component of an association rule is antecedant. THEN component is consequent.\n","\n","\n","*   Support = no: of transactions with x/total no of transactions\n","\n","Support(X->Y) = P(XUY)\n","To check how popular a given item is. Prob of an event to occur.\n","\n","*   Confidence(X->Y) = no: of transactions with x and y/no: of transactions with x = P(Y/X)\n","\n","To check how likely item Y is purchased when item X is purchased.\n","\n","*   Lift(X->Y) = Confidence(X->Y)/Support(Y)\n","              = Support(X intersection Y)/Support(X) * Support(Y)\n","\n","How likely Y is purchased when X is purchased while controlling for how popular Y is. Observed to expected ratio. This gives bidirectional recommendation. =1 if X and Y are indep. >1 means likely to be bought together. < 1 means unlikely to be bought together. Lift value lies in the range 0 to infinity.\n","\n","*   Leverage(X->Y) = Support(X intersection Y)- Support(X) * Support(Y)\n","\n","It computes the difference b/w the observed frequency of x and Y appearing together and the frequency that would be expected if X and Y were indep. =0 means independence. leverage value lies b/w -1 and +1\n","\n","*   Conviction(X->Y) = (1-Support(Y))/(1-Confidence(X->Y))\n","\n","Calculated as the ratio of the expected frequency that X occurs without Y if they were independent divided by the observed frequency of incorrect predictions. High conviction means consequent is highly dependent on antecedant.\n","\n","-> The Apriori Algorithm proposes that:\n","\n","The probability of an itemset is not frequent if:\n","\n","*   P(I) < Minimum support threshold, where I is any non-empty itemset\n","\n","*   Any subset within the itemset has value less than minimum support.\n","\n","The second characteristic is defined as the Anti-monotone Property. A good example would be if the probability of purchasing a burger is below the minimum support already, the probability of purchasing a burger and fries will definitely be below the minimum support as well.\n","\n","Its unsupervised. Doesn't need labelled data. But its computationally expensivve. Complexity grows exponentially. Cold start problem.\n","\n","**2. Content based filtering system**\n","\n","Makes recommendations based on similarity of items. It uses item features to recommend other items similar to what user likes based on previous actions or explicit feedback. Doesn't require other user's data. No cold start problem. Requires domain knowledge. Nevcer recommend anything from a different category.\n","\n","**3. Collaborative based filtering system**\n","\n","This uses similarities between users and items simultaneously to provide recommendations. Works on the assumption that people who like similar things have similar taste.\n","\n","-> Item-Item based rec s/m\n","\n","Similarity between items is calculated using hamming distance(no of bits differ). Users who liked this item also liked..\n","\n","-> User-User based\n","\n","Euclidean dist or cosine similarity is used to measure similarity of 2 users.\n","\n","Cosine similarity=a.b/||a||.||b||\n","\n","-> User-Item based\n","\n","user-item interaction matrix is formed.\n","\n","**4. Similarity based filtering system**\n","\n","**5. Matrix factorization**\n","\n","With the input of user's ratings on the items, we would like to predict how the users would rate the items so the users can get the recommendations based on the prediction.\n","\n","**6. Popularity based recommender system**\n","\n","## -> Performance measure\n","\n","1. Precision at k = no: of recommended items that are relevant in top k recommenders / No of recommended items in top k recommenders\n","\n","How good we are in retrieving relevant items.\n","\n","2. Recall at k = no: of recommended items that are relevant in top k recommenders / total no of relevant items"],"metadata":{"id":"bb0evQjBES3E"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JJdDhq_GEOVL"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# Time Series Analysis\n","\n","-> Time series data - Signal indexed by an ordered time stamp. For data to be time series we need to things - date/timestamp (t, eg: weeks,months), quantity (y, eg: sales,revenue,inventories)\n","\n","-> Forecasting - Analysing historical data to predict future values. Its a supervised learning problem.\n","\n","-> Linear interpolation is used for imputing missing values. Take the average of first pt before and after the missing value and fill the missing value with this average.\n","\n","-> Best way to detect anomaly is by plotting histogram. If histogram is more continous we can use the concept of quantiles to remove anomalies.\n","\n","-> Trend - Linear increasing or decreasing behaviour of the series over a long period. It usually happens for some time and then disappears. It doesnt repeat. uptrend, downtrend or can be up and down. Trend is calculated by taking moving average over a long period or by just fitting a Linear regression line on the points. We take average of last k data points and use it to guess the next point.\n","\n","-> Seasonality - Pattern that occurs at regular intervals. It occurs when a time series is affected by seasonal factors such as the time of the year or the day of week\n","\n","-> Time series decomposition\n","\n","We want to decompose signal into 3 main components - Trend, Seasonality, Residual\n","\n","Additive seasonality decomposition: y(t)=b(t)+s(t)+e(t) where b is trend, s is seasonality and e is error term of signal.\n","\n","Multiplicative seasonality decomposition: y(t)=b(t)*s(t)*e(t)\n","\n","-> Time based splitting is done for train test split.\n","\n","## Simple Forecast methods\n","\n","### 1. Mean forecast\n","\n","Take average of all past k values to predict value at t=k+1\n","\n","### 2. Naive approach\n","\n","Value at t=k is the value at t=k+1\n","\n","### 3. Seasonal naive forecast\n","\n","Value at t=k+1 will be the last observed value from the same season eg: same month of previous year.\n","\n","### 4. Drift method\n","\n","Take first and last point of data, draw a straight line, then extend into future (Linear extrapolation). Highly sensitive to last value available.\n","\n","### 5. Moving average\n","\n","Average of last k data points\n","\n","## Smoothing based methods\n","\n","### 1. Simple Exponential Smoothing (SES)\n","\n","More weightage to most recent data and less weightage for older data. alpha is the weight given to the most recent value. Its called smoothing parameter.\n","\n","y_hat_t+1 = alpha*y_t + alpha*(1-alpha)*y_t-1+alpha*(1-alpha)^2*y_t-2+..\n","\n","Level of forecasting is right. But its missing both trend and seasonality.\n","\n","### 2. Double Exponential Smoothing (DES)\n","\n","### 3. Triple Exponential Smoothing (Holt-Winters Method)\n"],"metadata":{"id":"xYuxBj1L2Hv4"}}]}